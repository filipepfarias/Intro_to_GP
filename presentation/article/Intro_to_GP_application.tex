% !TEX root = Intro_to_GP.tex
\section{Applications in disease mapping}
% \textcolor{red}{Here we must have defined how the inferencial process occurs.}
There's several applications using GP and here we'll resume an application for disease mapping. By \cite{Vanhatalo2010Vehtari} the problem concerns to use the latent Gaussian field and observe it indirectly from the data, aiming to infer a function for the phenomenon. For this scenario, the data are counts of mortality or morbidity occurrence per area and the pursued function is the relative risk for that area, what is modeled by a Poisson process discussed by \cite{Best2005}.

\subsection{The model}

The Poisson process is defined if the occurrence, this is the count of deaths $y$, in a given area $i$ is a Poisson variable and if the counting in one area does not affect the counting in another. The Poisson variable \ref{eq:observation-model} in each area is defined by its rate of the process $e_i\mu_i$ \cite{Best2005,Samat2012}, being 

\begin{equation}
    e_{i}=\sum_{r=1}^{R}\left(\frac{\sum_{i=1}^{n} y_{i, r}}{\sum_{i=1}^{n} p_{i, r}}\right) p_{i, r}
    \label{eq:standized-num-death}
\end{equation}

the standardized expected number of deaths and $\mu$ the relative risk of the area. In this work, the objective is to infer the relative risk given the data \cite{lawson2013statistical}. 
%
\begin{equation}
    y_i \sim Poisson(e_i\mu_i)
    \label{eq:observation-model}
\end{equation}
%
Then let's assume that our phenomenon according with $f=\log (\mu)$ \cite{Best2005}. So, we may say that we evaluated each observation $y_i$ is Poisson distributed and its mean is given by an unknown function $e_i e^{f_i}$. With this we assume that our observations and our functions are independent and then we can evaluate our joint distribution for the likelihood by the product of each one \cite{jarno2010}.

We'll model the log of the risk as a Gaussian process, then latent variables $f_i$ are realizations of the latent function $f$ at the inputs $\mathbf{x}_i$. The GP wll be defined by a covariance function $k(\mathbf{x},\mathbf{x}')$ and a mean function $m(\mathbf{x})$. For disease mapping, we can assume the mean being $\mathbf{m}=\mathbf{0}$, what implies that for a zero function, the risk will be one if there's no spatial variations. The covariance function has a collection of hyperparameters $\theta$.

We can represent with a graphical model the dependency of the variables, as in the \autoref{fig:graphical-model-disease}. Then, we can write the model as follows:
%
\begin{subequations}
     \begin{empheq}[left={\empheqlbrace\,}]{align}
      y_1, y_2, \dots, y_n &\sim \prod_{i=1}^{n} Poisson\left( e_i \exp (f_i) \right) \\
      f(\mathbf{x}) | \theta &\sim \mathcal{GP}\left( m(\mathbf{x}),k(\mathbf{x},\mathbf{x}'|\theta) \right) \\
      \theta &\sim \text{half-t}(\nu,A)\text{\footnotemark}
     \end{empheq}
 \end{subequations}\footnotetext{The values $\nu$ and $A$ are not arbitrary, but deterministic \cite{Vanhatalo2010Vehtari}.}
% 
\begin{figure}[tpb]
    \begin{center}
        \begin{tikzpicture}
            % Defining nodes
            \node[obs, fill=red!20!blue!30!white, semithick, draw=red]              (yn) {$y_n$};%
            \node[obs, left=of yn, fill=red!20!blue!30!white, semithick, draw=red]              (y3) {$y_3$};%
            \node[obs, left=of y3, fill=red!20!blue!30!white, semithick, draw=red]              (y2) {$y_2$};%
            \node[obs, left=of y2, fill=red!20!blue!30!white, semithick, draw=red]              (y1) {$y_1$};%
            \node[const, below=.5 of yn, xshift= -.8cm]                        (en) {$e_n$};
            \node[const, below=.5 of y3, xshift= -.8cm]                        (e3) {$e_3$};
            \node[const, below=.5 of y2, xshift= -.8cm]                        (e2) {$e_2$};
            \node[const, below=.5 of y1, xshift= -.8cm]                        (e1) {$e_1$};
            \node[latent, semithick, draw=red, below=of yn] (fn) {$f_n$};
            \node[latent, semithick, draw=red, below=of y3] (f3) {$f_3$};
            \node[latent, semithick, draw=red, below=of y2] (f2) {$f_2$};
            \node[latent, semithick, draw=red, below=of y1] (f1) {$f_1$};
            
            \node[latent, semithick, draw=red, right=of fn] (f*) {$f_*$};
            \node[const, semithick, left=.5 of f1] (f0) {};
            \node[const, semithick, right=.5 of f*] (f0s) {};

            % \node[obs, semithick, above=of f0 ,fill=red!20!blue!30!white, semithick, draw=red]              (y0) {};%
            % \node[obs, semithick, above=of f*, fill=red!20!blue!30!white, semithick, draw=red]              (y*) {$y_*$};%
            % \node[obs, semithick, below=of f0, fill=red!20!blue!30!white, semithick, draw=red]              (x0) {};%
            \node[obs, semithick, below=of fn, fill=white, semithick, draw=white]              (xn) {$\mathbf{x}_n$};%
            \node[obs, semithick, below=of f3, fill=white, semithick, draw=white]              (x3) {$\mathbf{x}_3$};%
            \node[obs, semithick, below=of f2, fill=white, semithick, draw=white]              (x2) {$\mathbf{x}_2$};%
            \node[obs, semithick, below=of f1, fill=white, semithick, draw=white]              (x1) {$\mathbf{x}_1$};%
            \node[obs, semithick, below=of f*, fill=white, semithick, draw=white]              (x*) {$\mathbf{x}_*$};%
            % \node[obs, semithick, above=of f0s ,fill=red!20!blue!30!white, semithick, draw=red]              (y0s) {};%

            % Hyperparameters

            \node[latent, semithick, draw=red, below= .5 of f0s, xshift= .3cm] (th) {$\theta$};
            \node[obs, draw=white, fill=white, below=.75 of th, xshift= -.5cm] (A) {$A$};
            \node[obs, draw=white, fill=white, below=.75 of th, xshift= .5cm] (nu) {$\nu$};


            % Connecting nodes
            \edge[draw=red, semithick] {en} {yn}; 
            \edge[draw=red, semithick] {e3} {y3}; 
            \edge[draw=red, semithick] {e2} {y2}; 
            \edge[draw=red, semithick] {e1} {y1}; 
            \edge[draw=red, semithick] {fn} {yn};
            \edge[draw=red, semithick] {f3} {y3};
            \edge[draw=red, semithick] {f2} {y2};
            \edge[draw=red, semithick] {f1} {y1};
            \edge[-, draw=red, line width=2.3pt] {f0} {f1};
            \edge[-, draw=red, line width=2.3pt] {f*} {fn};
            \edge[-, draw=red, line width=2.3pt] {f3} {f2};
            \edge[-, draw=red, line width=2.3pt] {f2} {f1};
            \edge[-, draw=red, dashed, line width=2.3pt] {f3} {fn};
            \edge[-, draw=red, line width=2.3pt] {f*} {f0s};
            % \edge[draw=red, semithick] {f0} {y0};
            % \edge[draw=red, semithick] {f*} {y*};
            \edge[draw=red, semithick] {x*} {f*};
            % \edge[draw=red, semithick] {x0} {f0};
            \edge[draw=red, semithick] {xn} {fn};
            \edge[draw=red, semithick] {x3} {f3};
            \edge[draw=red, semithick] {x2} {f2};
            \edge[draw=red, semithick] {x1} {f1};
            % \edge[draw=red, semithick] {th} {f0};
            \iffalse
            \edge[draw=red, semithick] {th} {f*};
            \edge[draw=red, semithick] {th} {f3};
            \edge[draw=red, semithick] {th} {f2};
            \edge[draw=red, semithick] {th} {f1};
            \edge[draw=red, semithick] {th} {fn};
            \fi
            
            \foreach \x in {f1,f2,f3,fn,f*}
	        \path (th) edge [draw=red, line width=.5pt, bend left=15,->]  (\x) ;
            
            \edge[draw=red, semithick] {nu} {th};
            \edge[draw=red, semithick] {A} {th};

            
            % Defining plates
            % \plate[draw=blue, very thick] {lkhd} {(en) (yn) (fn) (xn)} {$n$};
        \end{tikzpicture}
    \end{center}
    \caption{Graphical model for the GP for regression. Colored circles represent observed variables and whited ones represent the unknowns. The thick horizontal bar in $f_i$ node represents a set of fully connected nodes of the Gaussian field. Note that an observation $y_i$ is conditionally independent of all other nodes given the corresponding latent variable, $f_i$. Because of the marginalization property of GPs addition of further inputs, $\mathbf{x}$, latent variables, $f$, and unobserved targets, $y_*$, does not change the distribution of any other variables.}
    \label{fig:graphical-model-disease}
\end{figure}

\subsection{Inference and prediction}

We're interested in evaluate the conditional posterior of the latent variables $\mathbf{f}$, this is to express the uncertainty about the risk, given the knowledge about the number of deaths. We can write by the model in the \autoref{fig:graphical-model-disease} as
%
\begin{equation}
    p(\mathbf{f}|\mathcal{D},\theta) = \frac{p(\mathbf{f},\mathcal{D},\theta)}{p(\mathcal{D},\theta)},
    \label{eq:posterior-theta}
\end{equation}
%
being $\mathcal{D}=\left\{ \mathbf{X},\mathbf{y}, \mathbf{p} \right\}$. We can express $p(\mathcal{D},\theta)$ as $\int p(\mathbf{f},\mathcal{D},\theta)d\mathbf{f}$. By the graphical model we have that
%
\begin{equation}
    p(\mathbf{f},\mathcal{D},\theta) = p(\mathbf{y}|\mathbf{f},\mathbf{e})p(\mathbf{f}|\mathbf{X},\theta)p(\theta).
\end{equation}
%
The constant $\mathbf{e}$ is evaluated in \ref{eq:standized-num-death}, then its dependency is on $\mathbf{y}$ and $\mathbf{p}$. For the case of the expected number of deaths, we'll not consider $y$ being a random variable, just for the evaluation of $e$, then it's a constant. Finally we obtain the posterior for the latent values by marginalizing it, then from \ref{eq:posterior-theta} we have
{\color{red}
%
\begin{equation}
    p(\mathbf{f}|\mathcal{D})=\frac{\int p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|\mathbf{X},\theta)p(\theta) d\theta}{\int \int p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|\mathbf{X},\theta)p(\theta) d\theta d\mathbf{f}},
\end{equation}
%
}
omitting $\mathbf{e}$ for brevity.
{\color{red}
%

In this case, we used the Poisson distribution for the likelihood because the nature of the process. The phenomenon here is the relative risk of death $\mu$ in a region of the country. So, if we consider $y$ the counting of deaths on this region, we can model the phenomenon with a Poisson process \cite{Vanhatalo2010Vehtari}.

For numerical reasons, we transform $f=\log(\mu)$. Finally, we assume an uncertainty over the parameters $\theta$ of the kernel functions too, then, our hierarchical model stays for the posterior distribution as 
%
\begin{equation}
    p(\mathbf{f}|\mathbf{y},\mathbf{x}) \propto \int p(\mathbf{y}|\mathbf{f})\mathcal{GP}\left(\mathbf{f} | m(\mathbf{x}),k(\mathbf{x},\mathbf{x}'|\theta) \right)p(\theta) d\theta.
\end{equation}

This function isn't analytically tractable because of the Poisson process, but it is possible its evaluation with approximation methods.

}
