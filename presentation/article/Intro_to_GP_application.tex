% !TEX root = Intro_to_GP.tex
\section{Applications in disease mapping}
% \textcolor{red}{Here we must have defined how the inferencial process occurs.}
There's several applications using GP and here we'll resume an application for disease mapping. By \cite{Vanhatalo2010Vehtari} the problem concerns to use the latent Gaussian field and observe it indirectly from the data, aiming to infer a function for the phenomenon. For this scenario, the data are counts of mortality or morbidity occurrence per area and the pursued function is the relative risk for that area, what is modeled by a Poisson process discussed by \cite{Best2005}.

\subsection{The model}

The Poisson process is defined if the occurrence, this is the count of deaths $y$, in a given area $i$ is a Poisson variable and if the counting in one area does not affect the counting in another. 
%
\begin{equation}
    y_i \sim Poisson(e_i\mu_i)
    \label{eq:observation-model}
\end{equation}
%
The Poisson variable $y_i$ in each area is defined by its process rate $e_i\mu_i$ \cite{Best2005,Samat2012}, being 
%
\begin{equation}
    e_{i}=\sum_{r=1}^{R}\left(\frac{\sum_{i=1}^{n} y_{i, r}}{\sum_{i=1}^{n} p_{i, r}}\right) p_{i, r}
    \label{eq:standized-num-death}
\end{equation}
%
the standardized expected number of deaths and $\mu_i$ the relative risk of the area. The sum is weighted by social factors, dividing the population $p$ in groups $r$.  In this work, the objective is to infer this relative risk given the data \cite{lawson2013statistical}. 

Then let's assume $f=\log (\mu)$ \cite{Best2005}. So, we may say that we evaluated each observation $y_i$ is Poisson distributed and its mean is given by an unknown function $e_i \exp(f_i)$. With this we assume that our observations of the functions are independent and then we can evaluate the joint distribution for likelihood by the product of each one \cite{jarno2010}.

We'll model the log risk as a Gaussian process, then latent variables $f_i$ are realizations of the latent function $f$ at the inputs $\mathbf{x}_i$. The GP will be defined by a covariance function $k(\mathbf{x},\mathbf{x}')$ and a mean function $m(\mathbf{x})$. For disease mapping, we can assume the mean being $m(\mathbf{x})=\mathbf{0}$, what implies that for a zero function, the risk will be one if there's no spatial variations. The covariance function has a collection of hyperparameters $\theta$.

We can represent with a graphical model the dependency of the variables, as in the \autoref{fig:graphical-model-disease}. Then, we can write the model as follows:
%
\begin{subequations}
     \begin{empheq}[left={\empheqlbrace\,}]{align}
      y_1, y_2, \dots, y_n | \mathbf{f} &\sim \prod_{i=1}^{n} Poisson\left( e_i \exp (f_i) \right) \\
      f(\mathbf{x}) | \theta &\sim \mathcal{GP}\left(\mathbf{0},k(\mathbf{x},\mathbf{x}'|\theta) \right) \label{eq:gp-prior}\\
      \theta &\sim \text{half-Student-t}(\nu,A)\text{\footnotemark}
     \end{empheq}
 \end{subequations}\footnotetext{The values $\nu$ and $A$ are not arbitrary, but deterministic \cite{Vanhatalo2010Vehtari}.}
% 
\begin{figure}[tpb]
    \begin{center}
        %\input{graphics/GraphicalModel.tex} \\
        \input{graphics/GraphicalModel2.tex}
    \end{center}
    \caption{{\color{red}Graphical model for the GP for inference. Filled circles represent observed variables and whited ones represent the unknowns. The nodes without circles represent constants.}}
    \label{fig:graphical-model-disease}
\end{figure}
%
We're interested in evaluate the conditional posterior of the latent variable $\mathbf{f}$, i.e. to express the uncertainty about the risk given the knowledge about the number of deaths and the hyperparameters. We can write by the model in the \autoref{fig:graphical-model-disease}, the following conditional
%
\begin{equation}
    p(\mathbf{f},\mathcal{D},\theta) = p(\mathbf{y}|\mathbf{f},\mathbf{e})p(\mathbf{f}|\mathbf{X},\theta)p(\theta).
    \label{eq:model-total-density}
\end{equation}
%
being $\mathcal{D}=\left\{ \mathbf{X},\mathbf{y}, \mathbf{p} \right\}$. The constant $\mathbf{e}$ evaluated in \eqref{eq:standized-num-death} depends of $\mathbf{y}$ and $\mathbf{p}$, then we can omit from the probability function. Note that it isn't considered as a random variable even $y$ being a random. This occurs because the evaluation of $e_i$ needs some information about $y$ as in which social group $r$ the death was counted \eqref{eq:standized-num-death}, what can't be modeled if $y$ was treated as random without rise the complexity of the model. This would lead to a second inference that would be the classification of $y$.
%
Finally we obtain the posterior for the latent values by marginalizing it, then we have
%
\begin{equation}
    p(\mathbf{f}|\mathcal{D})=
    \frac{p(\mathbf{f},\mathcal{D})}{p(\mathcal{D})}=
    \frac{\int p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|\mathbf{X},\theta)p(\theta) d\theta}{\int \int p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|\mathbf{X},\theta)p(\theta) d\mathbf{f} d\theta},
 \label{eq:model-posterior-density}
\end{equation}
%
omitting the constant $\mathbf{e}$ for brevity. The quantity $p(\mathcal{D})$ can be represented by the marginalization of $p(\mathbf{f},\mathcal{D})$ with respect to $\mathbf{f}$, i.e. $\int p(\mathbf{f},\mathcal{D}) d\mathbf{f}$. This integral can't be evaluated analytically, then we use approximation methods to do this.
%

\subsection{Conducting the inference}

In the last part, we defined the model for the the observations $f$. Now we have to obtain the posterior distribution $p(\mathbf{f}|\mathcal{D})$, considering the value of $\theta$ that maximizes our posterior distribution and conduct the inference. This maximization can be done analytically with approximations of $p(\theta|\mathcal{D})$ or by simulations considering the whole distribution, what is much slower \cite{Vanhatalo2010Vehtari}. Then by the first alternative we have $p(\mathbf{f}|\mathcal{D},\hat{\theta})$ being $\hat{\theta}$ the one that maximizes the posterior. For it, we do
%
\begin{equation}
\hat{\theta} = \underset{\theta}{\operatorname{arg \ max}} \ p(\theta|\mathcal{D}).
\end{equation}
%
It's important to note that the distribution $p(\theta|\mathcal{D})$ is justified by the fact that the distribution of $\theta$ rules the kernel of the GP, then there is a relation between the variables, as  \eqref{eq:gp-prior} shows, that was not described in the Figure \ref{fig:graphical-model-disease}. By \eqref{eq:model-total-density} we obtain
%
\begin{equation}
p(\mathcal{D}|\theta)=\int \frac{p(\mathbf{f},\mathcal{D},\theta)}{p(\theta)}d\mathbf{f}= \int p(\mathbf{y}|\mathbf{f},\mathbf{e})p(\mathbf{f}|\mathbf{X},\theta) d\mathbf{f}
\end{equation}
%