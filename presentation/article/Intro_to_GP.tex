% !TeX root = ./Intro_to_GP.tex
\documentclass[11pt]{article} % For LaTeX2e
\usepackage{Intro_to_GP}

\title{Introduction to Gaussian Processes}

\IntroGPfinaltrue

\author{
Filipe P.~Farias \\
Teleinformatics Engineering Department\\
Federal University of Ceará\\
\texttt{filipepfarias@fisica.ufc.br} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
   A wide variety of methods exists to deal with supervised learning, as restrict a class of linear functions of the inputs, as linear regression, or give a prior probability to every possible function, giving high probability to the functions we consider more likely. The second approach is a way to Gaussian process itself. We will make the pathway through a intuitive construction of this framework.
\end{abstract}

\section{Introduction}

\lipsum[1]%

\section{Linear Regression}

Starting with a simple regression problem. Be the dataset $\mathrm{D}=\left\{ x_i,y_i|i=1,\dots,N \right\}$, where we observe a real-valued input variable $x$ and a measured real-valued variable $y$. Then, we'll use synthetically generated data for comparison against any learned \textit{model}. And $N$ will be the number of observations of the value $y$. Our objective is make predictions of the new value $\hat{y}$ for some new input $\hat{x}$.

For this example, we'll use a simple approach based on curve fitting by the polynomial model, i.e, being the function

\begin{equation}
   f(x,\mathbf{w}) = \sum_{j=0}^M w_j x^j
\end{equation}

where $M$ is the order of the polynomial and $\mathbf{w}=\left[ w_0,\dots,w_M \right]$ its coefficients. It's important to note that the $f$ isn't linear in $x$ but in $\mathbf{w}$. These functions which are linear on the unknown parameters are called \textit{linear models}.
\textcolor{red}{Section 1.1 - Bishop (pg 4).}

We can extend the class of models considering linear combinations of nonlinear functions of the input variables, i.e

\begin{equation}
   f(x,\mathbf{w}) = \sum_{j=0}^{M-1} w_j \phi_j (x)
   \label{eq:lin-reg-model-1}
\end{equation}

where $\phi_j (x)$ are known as \textit{basis functions}, and then the total number of parameters for this model will be $M$. We can evaluate the same operation of (\ref{eq:lin-reg-model-1}) in the matrix form by

\begin{equation}
   f(x,\mathbf{w}) = \mathbf{w}^\top \boldsymbol{\phi}(x)
   \label{eq:lin-reg-model-1-matrix-form}
\end{equation}

where $\boldsymbol{\phi}(x) = \left[ \phi_0(x), \dots, \phi_{M-1}(x) \right]^\top$. In the example of the curve fitting, the polynomial regression implies that $\phi_j(x)=x^j$. It's important to note that these linear models are needed to define its basis functions before the training dataset is observed. \textcolor{red}{Section 1.4 - Bishop (pg 33).}

There are many example of choices for basis functions, as

\begin{equation}
   \phi_j(x) = \exp \left\{ -\frac{\left(x-\mu_j\right)^2}{2s^2} \right\}
\end{equation}

known as \textit{squared exponential}, where $\mu_j$ controls the location of the basis function in the \textit{input space}, and $s$ the spatial scale. It's usually referred as 'Gaussian' basis function because of its similarity with the Gaussian distribution function, although there is no probabilistic interpretation here.

Backing to the regression problem
% \begin{figure}[H]
%    \centering
%    \includegraphics[width=.25\textwidth]{Figures/f1.png}
%    \caption{Símbolos do diagrama de blocos. (a) Adição de duas seqüências. (b) Multiplicação de uma sequência por um  constante. c) atraso da unidade.}
%    \label{graph:A-and-B-systems}
% \end{figure}

\lipsum[1]

\end{document}
