% !TeX root = ./Intro_to_GP.tex
\documentclass[11pt]{article} % For LaTeX2e
\usepackage{Intro_to_GP}

\title{Introduction to Gaussian Processes}

\IntroGPfinaltrue

\author{
Filipe P.~Farias \\
Teleinformatics Engineering Department\\
Federal University of Ceará\\
\texttt{filipepfarias@fisica.ufc.br} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
   A wide variety of methods exists to deal with supervised learning, as restrict a class of linear functions of the inputs, as linear regression, or give a prior probability to every possible function, giving high probability to the functions we consider more likely. The second approach is a way to Gaussian process itself. We will make the pathway through a intuitive construction of this framework.
\end{abstract}

\section{Introduction}

\lipsum[1]%
\nocite{Bishop:2006:PRM:1162264}
\nocite{Rasmussen:2005:GPM:1162254}

\section{Linear Regression}

Starting with a simple regression problem. Be the data set $\mathcal{D}=\left\{ x_i,y_i|i=0,\dots,N-1 \right\}$, where we observe a real-valued input variable $x$ and a measured real-valued variable $y$. Then, we'll use synthetically generated data for comparison against any learned \textit{model}. And $N$ will be the number of observations of the value $y$. Our objective is make predictions of the new value $\hat{y}$ for some new input $\hat{x}$.

For this example, we'll use a simple approach based on curve fitting by the polynomial model, i.e., being the function

\begin{equation}
   f(x,\mathbf{w}) = \sum_{j=0}^{M-1} w_j x^j
\end{equation}

where $M$ is the order of the polynomial and $\mathbf{w}=\left[ w_0,\dots,w_M \right]$ its coefficients. It's important to note that the $f$ isn't linear in $x$ but in $\mathbf{w}$. These functions which are linear on the unknown parameters are called \textit{linear models}.
\textcolor{red}{Section 1.1 - Bishop (pg 4).}

We can extend the class of models considering linear combinations of nonlinear functions of the input variables, i.e.

\begin{equation}
   f(x,\mathbf{w}) = \sum_{j=0}^{M-1} \phi_j (x) w_j 
   \label{eq:lin-reg-model-1}
\end{equation}

where $\phi_j (x)$ are known as \textit{basis functions}, and then the total number of parameters for this model will be $M$. We can evaluate the same operation of (\ref{eq:lin-reg-model-1}) in the matrix form by

\begin{equation}
   f(x,\mathbf{w}) = \boldsymbol{\phi}(x)^\top  \mathbf{w}
   \label{eq:lin-reg-model-1-matrix-form}
\end{equation}

where $\boldsymbol{\phi}(x) = \left[ \phi_0(x), \dots, \phi_{M-1}(x) \right]^\top$. In the example of the curve fitting, the polynomial regression implies that $\phi_j(x)=x^j$. It's important to note that these linear models are needed to define its basis functions before the training data set is observed. \textcolor{red}{Section 1.4 - Bishop (pg 33).}

The values of $\mathbf{w}$ are obtained by minimizing the \textit{error function}, a measure of the distance between the training data set and $f$, given values of $\mathbf{w}$. By the way, the chosen error function will be 

\begin{equation}
   E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\{ f(x_i,\mathbf{w})-y_i \right\}^2
   \label{eq:lin-reg-error-function-1}
\end{equation}

This indicate that if $E$ is zero, $f$ passes exactly through each training data point. Observe that $E$ do not assume negative values because of its quadratic form, then we can find $\mathbf{w}$ by finding the minimum value of $E$, denoted $\mathbf{w^*}$, by

\begin{equation}
   \frac{\partial E}{\partial \mathbf{w}} = 0
\end{equation}

We can rewrite (\ref{eq:lin-reg-error-function-1}) in the matrix form, considering $\mathbf{f} = f(\mathbf{x},\mathbf{w})$, where $\mathbf{x}=\left[ x_0,\dots, x_{N-1} \right]^\top$, i.e. $f$ evaluated for all input variables (See Appendix \ref{subsec:app-matrix-form}). Then we have

\begin{equation}
   \label{eq:lin-reg-error-function-matrix-form}
   \mathbf{f} = \Phi \mathbf{w}
\end{equation}

where $\Phi$ is the \textit{design matrix} such that $\boldsymbol{\phi}(x)$ is evaluated for all $\mathbf{x}$. Proceeding with the minimization, we obtain the optimal $\mathbf{w}$, or $\mathbf{w}^*$ by

\begin{equation}
   \label{eq:lin-reg-opt-param}
   \mathbf{w}^{*} = (\Phi^\top \Phi)^{-1}\Phi^\top \mathbf{y}
\end{equation}

which are the parameters that best fit the model to the data.

\textcolor{red}{Insert over-fitting.}

As we increase the number of parameters, our model becomes more flexible and then our error function approximates of zero for the training data. But when compared to the test data, the error increases. This is known as \textit{over-fitting}.

\subsection{Regularized Linear Regression}

An approach to minimize the over-fitting problem is to control the flexibility of the model. This can be done by controlling the norm of $\mathbf{w}^*$ as the number of parameters increases. By (\ref{eq:lin-reg-error-function-1}) we can add the penalty term $||\mathbf{w}||^2 $ scaled by the factor $\lambda / 2$, then

\begin{equation}
   \label{eq:lin-reg-error-function-regularized}
   E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\{ f(x_i,\mathbf{w})-y_i \right\}^2 + \frac{\lambda}{2} ||\mathbf{w}||^2
\end{equation}

whats means that our error increases as the norm of $\mathbf{w}$ grows. This will lead us to the matrix form

\begin{equation}
   \label{eq:lin-reg-error-function-regularized-matrix-form}
   \mathbf{w}^{*} = (\Phi^\top \Phi + \lambda \mathbf{I})^{-1}\Phi^\top \mathbf{y} 
\end{equation}

This allow us to increase the number of parameters trying to control the over-fitting. More, add parameters will allows us to capture different aspects of the data set, what we will see later.

\section{Bayesian Linear Regression}

\subsection{A Bayesian view of Linear Regression}

Until now, we see the curve fitting problem in terms of error minimization. Then we will see the same by a probabilistic perspective gaining some insights into error minimization and regularization, leading us to a full Bayesian treatment.

\begin{equation}
   \label{eq:bay-reg-bayes-rule}
   p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}
\end{equation}

We can use the Bayes' theorem (\ref{eq:bay-reg-bayes-rule}) to convert a \textit{prior} probability into a \textit{posterior} probability at the light of some evidence. We can make inferences about quantities such as  the parameters $\mathbf{w}$ in the form of a prior distribution $p(\mathbf{w})$. The observation of the data $\mathcal{D}$ and what its implies in the parameters is expressed as a conditional probability $p(\mathcal{D}|\mathbf{w})$. Then we can evaluate the uncertainty about $\mathbf{w}$ after observed the data $\mathcal{D}$ as a posterior probability $p(\mathbf{w}|\mathcal{D})$.

The quantity $p(\mathcal{D}|\mathbf{w})$ expresses how probable the observed data $\mathcal{D}$ is for different settings of $\mathbf{w}$. Then, not being a probability distribution over the parameters, its integral with respect to $\mathbf{w}$ could not be equal one, then to normalize the equation with respect to the left-side there's a term $p(\mathcal{D})$. This distribution is called \textit{likelihood function}.

Integrating the both sides with respect to $\mathbf{w}$, we obtain the denominator, then considering that integrating a probability distribution over itself is equal to one, we have

\begin{equation}
   \label{eq:bay-lin-bayes-theorem}
   p(\mathcal{D})=\int p(\mathcal{D} | \mathbf{w}) p(\mathbf{w}) \mathrm{d} \mathbf{w}
\end{equation}

\subsection{Bayesian curve fitting}

Let's consider the same data set $\mathcal{D}$ presented before, but now we have some uncertainty over the value of the measured value $y$. This uncertainty can be represented as a probability distribution $p$, in this particular case a Gaussian distribution, with a mean equal to the model $f(x,\mathbf{w})$. Thus we have

\begin{equation}
   \label{eq:bay-cur-fit-1}
   p(y | x, \mathbf{w}, \beta)=\mathcal{N}\left(y | f(x, \mathbf{w}), \beta^{-1}\right)
\end{equation}

Where $\beta$ is the variance of the distribution. Note that a large $\beta$ will give is more uncertainty about the measured value $y$, then we can call it of \textit{precision parameter}, i.e. how much certain we are about $y$. As we done in linear regression, we are trying to obtain the parameters for the model. In other words, given a value $y$, we trying to obtain the \textit{mean} and the \textit{variance} which maximize the probability of the measured value. Assuming the data set being independent and identically distributed, the joint probability of the whole data set will be

\begin{equation}
   p(\mathbf{y} | \mathbf{x}, \mathbf{w}, \beta)= \prod_{i=0}^{N-1} \mathcal{N}\left(y_i | f(x_i, \mathbf{w}), \beta^{-1}\right)
\end{equation}

When viewed as function of $f(x_i, \mathbf{w})$, the model, and $\beta^{-1}$, this is the likelihood function for the Gaussian. The parameters of the distribution can be determined by maximizing the likelihood function. It is convenient to maximize the log of the likelihood function, or minimize the negative log whats is equivalent, this implies that the maximization of the log of the function is equivalent to the maximization of the function itself, because the logarithm is a monotonically increasing of its argument. This helps the mathematical analysis and helps numerically because the small probabilities can easily underflow the numerical precision of the computer. Then\footnote[2]{Consider the Gaussian distribution as $\mathcal{N}\left(x | \mu, \sigma^{2}\right)=\left(2 \pi \sigma^{2}\right)^{-1 / 2} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}$}

\begin{equation}
   \label{eq:bay-reg-ml-function}
   \ln p(\mathbf{y} | \mathbf{x}, \mathbf{w}, \beta)=-\frac{\beta}{2} \sum_{i=1}^{N}\left\{f\left(x_{i}, \mathbf{w}\right)-y_{i}\right\}^{2}+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)
\end{equation}

The maximization of (\ref{eq:bay-reg-ml-function}) taking the derivative with respect to $\mathbf{w}$ will lead us back to 
the same of the minimization of (\ref{eq:lin-reg-error-function-1}), the error function of the linear regression. Here, just by notation, we will call the resulting parameters of the maximization of $\mathbf{w}_{\text{ML}}$, what it is called \textit{maximum likelihood}.

We can determine the precision parameter using the maximum likelihood by taking the derivative with respect to $\beta$ of (\ref{eq:bay-reg-ml-function}), what gives

\begin{equation}
   \frac{1}{\beta_{\mathrm{ML}}}=\frac{1}{N} \sum_{i=0}^{N-1}\left\{f\left(x_{i}, \mathbf{w}_{\mathrm{ML}}\right)-y_{i}\right\}^{2}
\end{equation}

Now we have a probabilistic view of the regression and then we can make predictions for new values of $x$, given that our model is capable of learn the parameters. And not just one collection of them, but a distribution probability.

In other words, after find the maximum likelihood parameters $\mathbf{w}_\text{ML}$ and $\beta_\text{ML}$, we have the parameters distribution by

\begin{equation}
   p\left(y | x, \mathbf{w}_{\mathrm{ML}}, \beta_{\mathrm{ML}}\right)=\mathcal{N}\left(y | f\left(x, \mathbf{w}_{\mathrm{ML}}\right), \beta_{\mathrm{ML}}^{-1}\right)
\end{equation}

Aiming to apply a "more Bayesian" approach, we not have yet a prior distribution to make the inference using the Bayes' rule. We can now introduce here the probability distribution over the parameters $p(\mathbf{w})$ as presented in the Section 3.1. The choice is arbitrary, but for this particular case we will consider

\begin{equation}
   \label{eq:bay-lin-reg-prior-dist}
   p(\mathbf{w} | \alpha)=\mathcal{N}\left(\mathbf{w} | \mathbf{0}, \alpha^{-1} \mathbf{I}\right)=\left(\frac{\alpha}{2 \pi}\right)^{(M+1) / 2} \exp \left\{-\frac{\alpha}{2} \mathbf{w}^\top \mathbf{w}\right\}
\end{equation}

where $\alpha$ is the variance, or precision parameter, of the distribution and $M+1$ is the number of parameters of the model, i.e. the length of $\mathbf{w}$. We call \textit{hyperparameters} the variables such $\alpha$ who control the model parameters distribution. And now we have by the Bayes' theorem considering that our \textit{posterior} distribution is proportional to the product between the \textit{likelihood function} and the assumed \textit{prior}, as seen in (\ref{eq:bay-lin-bayes-theorem}) then, assuming the observation of the whole data set

\begin{equation}
   p(\mathbf{w} | \mathbf{x}, \mathbf{y}, \alpha, \beta) \propto p(\mathbf{y} | \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w} | \alpha)
\end{equation}

As done before, we maximize the posterior probability, i.e. find the most probable value given the data by the term $p(\mathbf{w} | \mathbf{x}, \mathbf{y}) $ aside of the distribution parameters. This will result a particular choice of $\mathbf{w}$. We call this approach of \textit{maximum posterior}, or MAP. Then taking the negative logarithm
%
\begin{equation}
   \ln p(\mathbf{w} | \mathbf{x}, \mathbf{y}, \alpha, \beta) \propto \ln p(\mathbf{y} | \mathbf{x}, \mathbf{w}, \beta) + \ln p(\mathbf{w} | \alpha)
\end{equation}
%
Then we substitute the probability distributions founded before. Note that the first term in the right side is the error function founded in (\ref{eq:bay-reg-ml-function}). Then, the terms which the minimization depends of $\mathbf{w}$ are

\begin{equation}
    \frac{\beta}{2} \sum_{i=1}^{N}\left\{f\left(x_{i}, \mathbf{w}\right)-y_{i}\right\}^{2} + \frac{\alpha}{2} \mathbf{w}^\top \mathbf{w}
\end{equation}

And we can note the similarity with the regularized linear regression in (\ref{eq:lin-reg-error-function-regularized}) aside of the term $\lambda$, what can be founded by $\lambda=\alpha / \beta$. It's important to note that even called maximum posterior, here it was presented a minimization in terms of the negative logarithm, but this equals to the maximization of the positive logarithm. The signal was chosen just for similarity with the error function.

\subsection{Bayesian inference}

The similarity mentioned before shows that the Bayesian approach comprise even a model training such as the classical regression, as also the control of the over fitting by the regularization. But to say that our model is in fact Bayesian, we might obtain not just a single value, as in MAP, but its distribution. This requires the application the fully Bayes' theorem as (\ref{eq:bay-reg-bayes-rule}). Then, we have

\begin{equation}
   \overbrace{p(\mathbf{w}|\mathbf{y})}^{posterior} = \frac{p(\mathbf{y}|\mathbf{w}) p(\mathbf{w})}{p(\mathbf{y})} = \frac{\overbrace{p(\mathbf{y}|\mathbf{w})}^{\text{\textit{likelihood}}} \overbrace{p(\mathbf{w})}^{\text{\textit{prior}}}}{\underbrace{\int p(\mathbf{y}|\mathbf{w}) p(\mathbf{w}) d\mathbf{w}}_{\text{\textit{marginal distribution}}}}
\end{equation}

This is called \textit{Bayesian inference}. If we assume that all distributions which we are working are Gaussian, the posterior distribution has closed form. To do that, we make use of the closure under linear transformations, or \textit{affine transformations}, for the Gaussian. For that, we will make use of the corollary below, which theorems are proven in the \autoref{subsec:app-par-gau}.

\begin{corollary}
   \label{cor:aff-marg-cond-gaussian}

   Being $\mathbf{x}_b$ conditioned on $\mathbf{x}_a$ and Gaussian distributed as

   \begin{equation}
     p\left(\mathbf{x}_{a}\right)=\mathcal{N}\left(\mathbf{x}_{a} | \boldsymbol{\mu}_{a}, \boldsymbol{\Sigma}_{a}\right), \quad p\left(\mathbf{x}_{b} | \mathbf{x}_{a}\right)=\mathcal{N}\left(\mathbf{x}_{b} | \mathbf{M} \mathbf{x}_{a}+\mathbf{d}, \boldsymbol{\Sigma}_{b | a}\right)
   \end{equation}

   with $\boldsymbol{\mu}_{b} =\mathbf{M} \boldsymbol{\mu}_{a}+\mathbf{d}$, $\boldsymbol{\Sigma}_{b} =\boldsymbol{\Sigma}_{b | a}+\mathbf{M} \boldsymbol{\Sigma}_{a} \mathbf{M}^\top$, $\mathbf{M}$ a constant matrix and $\mathbf{d}$ a constant vector, both with the appropriate dimensions. Then conditional distribution $p(\mathbf{x}_a|\mathbf{x}_b)$ is given by

   \begin{subequations}
   
   \begin{align}
     p\left(\mathbf{x}_{a} | \mathbf{x}_{b}\right)&=\mathcal{N}\left(\mathbf{x}_{a} | \boldsymbol{\mu}_{a | b}, \boldsymbol{\Sigma}_{a | b}\right)
   \end{align}
   with
   \begin{align}
       \boldsymbol{\mu}_{a | b}&=\boldsymbol{\Sigma}_{a | b}\left(\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1}\left(\mathbf{x}_{b}-\mathbf{d}\right)+\boldsymbol{\Sigma}_{a}^{-1} \boldsymbol{\mu}_{a}\right) \\ \boldsymbol{\Sigma}_{a | b}&=\left(\boldsymbol{\Sigma}_{a}^{-1}+\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}\right)^{-1}.
   \end{align}
 \end{subequations}
 \end{corollary}

 Assuming no deviation in the mean, $\mathbf{d} = \mathbf{0}$, and the linear transformation being our design matrix, $\mathbf{M} = \Phi$, we obtain that

 \begin{equation}
   \label{eq:bay-inf-posterior}
    p(\mathbf{w}|\mathbf{y},\alpha,\beta) = \mathcal{N} \left( \mathbf{w} | \boldsymbol{\mu}_{\mathbf{w}|\mathbf{y}} , \boldsymbol{\Sigma}_{\mathbf{w}|\mathbf{y}}\right)
 \end{equation}

 being

 \begin{equation}
   \boldsymbol{\mu}_{\mathbf{w}|\mathbf{y}}=\boldsymbol{\Sigma}_{\mathbf{w}|\mathbf{y}}\left(\beta \Phi^\top \mathbf{y}+\boldsymbol{\Sigma}_{\mathbf{w}}^{-1} \boldsymbol{\mu}_{\mathbf{w}}\right), \quad \boldsymbol{\Sigma}_{\mathbf{w}|\mathbf{y}}=\left(\boldsymbol{\Sigma}_{\mathbf{w}}^{-1}+ \beta \Phi^\top \Phi\right)^{-1}
 \end{equation}

 assumed the prior distribution defined in (\ref{eq:bay-lin-reg-prior-dist}) and the precision matrix $\boldsymbol{\Sigma}_{\mathbf{t}|\mathbf{w}} = \beta^{-1}\mathbf{I}$. Then we have defined

 \begin{equation}
    \boldsymbol{\mu}_\mathbf{w}=\mathbf{0}, \quad \boldsymbol{\Sigma}_\mathbf{w} = \alpha^{-1} \mathbf{I}
 \end{equation}

\subsection{Predictive distribution}

In practice, some times it is more valuable the information about $y$ itself than its parameters $\mathbf{w}$. We can make this by evaluating the predictions of $y$ for the new values of $x$ by

\begin{equation}
   p(y | \mathbf{y}, \alpha, \beta)=\int p(y | \mathbf{w}, \beta) p(\mathbf{w} | \mathbf{y}, \alpha, \beta) \mathrm{d} \mathbf{w}
\end{equation}

what is called \textit{predictive distribution}. The disrtibutions under integration were defined in (\ref{eq:bay-cur-fit-1}) and (\ref{eq:bay-inf-posterior}). Then we use the \textit{marginalization} defined in \autoref{subsec:app-par-gau} and obtain that

\begin{equation}
   p(y | \mathbf{x}, \mathbf{y}, \alpha, \beta)=\mathcal{N}\left(y | \boldsymbol{\mu}_{y}, \boldsymbol{\Sigma}_{y} \right)
\end{equation}

with 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There are many example of choices for basis functions, as

\begin{equation}
   \phi_j(x) = \exp \left\{ -\frac{\left(x-\mu_j\right)^2}{2s^2} \right\}
\end{equation}

known as \textit{squared exponential}, where $\mu_j$ controls the location of the basis function in the \textit{input space}, and $s$ the spatial scale. It's usually referred as 'Gaussian' basis function because of its similarity with the Gaussian distribution function, although there is no probabilistic interpretation here.

% \begin{figure}[H]
%    \centering
%    \includegraphics[width=.25\textwidth]{Figures/f1.png}
%    \caption{Símbolos do diagrama de blocos. (a) Adição de duas seqüências. (b) Multiplicação de uma sequência por um  constante. c) atraso da unidade.}
%    \label{graph:A-and-B-systems}
% \end{figure}

\lipsum[1]

\newpage
\input{Intro_to_GP_appendices}

\newpage
\bibliography{Intro_to_GP_bibliography}

\end{document}