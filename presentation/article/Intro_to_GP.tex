% !TeX root = ./Intro_to_GP.tex
\documentclass[11pt]{article} % For LaTeX2e
\usepackage{Intro_to_GP}

\title{Introduction to Gaussian Processes}

\IntroGPfinaltrue

\author{
Filipe P.~Farias \\
Teleinformatics Engineering Department\\
Federal University of Cear√°\\
\texttt{filipepfarias@fisica.ufc.br} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
   The Gaussian processes have proven to be a powerfull framework for robust estimation and a flexible model for non-linear \textit{regression}, case which will be the main object of this work, with some implementations of real situations.% \cite{Rasmussen_2004}
\end{abstract}

\section{Introduction}
\nocite{hennig2013gaussian}

First, we`ll overview some initial concepts that will set the background for the GP. Let's suppose a data set $\mathcal{D} = \left\{ \left( \mathbf{x}_i,\mathbf{t}_i \right) \right\}_1^N$ which we denote the input as $\mathbf{x}$, the output (or target) as $\mathbf{t}$ and $N$ as the number of observations. The first step in our workflow is define a \textit{training} set, i.e. some data that is given to make our first assumptions of the \textit{model}. The model $\mathbf{y}$ can be defined as a guess of the law that rules the phenomenon of which our data was observed. This law can be, by example a senoid function as represented in \textcolor{red}{Figure 1}. Given this training data we wish to make \textit{predictions} for new inputs $\mathbf{x_*}$ that we have not observed in the training set.

% \begin{equation}
%    y(\mathbf{x},\mathbf{w}) = \mathbf{x}^\top \mathbf{w}
% \end{equation}

\textcolor{red}{We'll assume an parametric approach, then the model is said to contain \textit{parameters} $\mathbf{w}$, that will be adjusted during the \textit{training phase}}, when those are modified aiming to reduce the mismatch with the training set. In general we define a \textit{loss function} $L\left( y,t \right)$ which increases itself as the mismatch becomes larger, in other words the \textit{error} of the model. Then our work will be to reduce this error such that the smallest one will be the which defines when our model has learned the parameters of the law of the phenomenon. This turns possible to make our predictions where the data was not observed.

% \begin{equation}
%    L\left( y,t \right)
% \end{equation}

Unfortunately, in this trying of obtain the model by the smallest error, we may lose the capability of generalize it, i.e. our model could learned well for the training set only. So, if new data arrive or a new realization of the phenomenon occurs, that smallest error may increase for the same model. With this we define that our model isn't flexible. Then we can increase this flexibility by accepting some \textit{uncertainty} above it. More, sometimes a good first assumption can make the difference to the estimation, and one may want to put its beliefs in the model even before to observe the data, i.e. make a \textit{prior} assumption. These both strategies of uncertainty and prior assumptions are well defined by the Bayesian inference and could help if we assume a \textit{probabilistic model}.

The Bayesian inference can handle with the classical approaches of search the model which has the smallest error, but our objective is achieve one step ahead. We can not only obtain one model, but a \textit{distribution} of possible models. And with this, all the probabilistic meaning of distribution is carried with it, that is we can obtain both the model which \textit{minimize} the error or the statistics of the distribution of models. A more explaining view of what this really means will be given in the next sections.

Furthermore, the concept of \textit{infer} is similar to what we have done since beginning. We maded a guess of the law which rules the phenomenon, i.e. a prior assumption. Then we turns our model more plausible by reducing its error, or more \textit{likely}. After, we obtained a result of these assumptions, a \textit{posterior} assumption. These steps are similar in concept when dealing with Bayesian inference,  except that, as we will deal with probability distributions, then some rules must be established for the method to be concise.

Finally, we'll deal with a specific class of models in which we assume not a distribution of parameters but functions in general. By example, in a space with infinite possible functions, we'll evaluate how much possible which one are to be generated the data by its statistics, what is similar to what was done for the parameters. And in this part we make the fully use of the Gaussian process.

\section{Bayesian Linear Regression}

We start considering a model $y\left( \mathbf{x},\mathbf{w} \right)$ parametrized by $\mathbf{w}$

\begin{figure}[htbp]
   \centering
   \includegraphics{graphics/prml/{Figure1.2}.pdf}
   \caption{Training data set with $n=10$ points in blue. The green curve shows the function $sin(2\pi x)$ used to generate the data. Our goal is to predict the value of t for some new value of x, without knowledge of the green curve \cite{Bishop:2006:PRM:1162264}.}
   % \label{<label>}
\end{figure}

% Thus we need to move from the finite training data $\mathcal{D}$ to a function $f$ that makes predictions for all possible input values. To do this we must make assumptions about the characteristics of the underlying function, as otherwise any function which is consistent with the training data would be equally valid.


The approach is to give a \textit{prior} probability to every possible function, where higher probabilities are given to functions that we consider to be \textit{more likely}, for example because they are smoother than other functions. \cite{Rasmussen:2005:GPM:1162254}. This appears to have a serious problem, in that surely there are an infinite set of possible functions to compute in a finite time. This is where the Gaussian \textit{process} comes as a possible approach. 
A Gaussian process is a generalization of the Gaussian probability \textit{distribution}. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a random process governs the properties of functions \cite{getoor2009}. 

We can think of a function as a very long vector, each entry in the vector specifying the function value $f(x)$ at a particular input $x$. Indeed, the question of how we deal computationally with these infinite dimensional objects has the resolution: if you ask only for the properties of the function at a finite number of points, then \textit{infer} the properties for the predictions in the Gaussian process will give you the same answer if you ignore the infinitely many other points, as if you would have taken them all into account. One of the main attractions of the Gaussian process framework is precisely that it unites a sophisticated and consistent view with computational tractability.

\section{Linear Regression}

To understand the concepts the are covered by the Gaussian processes, we start with a simple regression problem. Be the data set $\mathcal{D}=\left\{ x_i,t_i|i=0,\dots,N-1 \right\}$, where we observe a real-valued input variable $x$ and a measured real-valued variable $t$. Then, we'll use synthetically generated data for comparison against any learned \textit{model}. And $N$ will be the number of observations of the value $t$. Our objective is make predictions of the new value $\hat{t}$ for some new input $\hat{x}$.



For this example, we'll use a simple approach based on curve fitting by the polynomial model, i.e., being the function

\begin{equation}
   \label{eq:lin-reg-model-1}
   y(x,\mathbf{w}) = \sum_{j=0}^{M-1} w_j x^j
\end{equation}

where $M$ is the order of the polynomial and $\mathbf{w}=\left[ w_0,\dots,w_M \right]$ its coefficients. It's important to note that the $y$ isn't linear in $x$ but in $\mathbf{w}$. These functions which are linear on the unknown parameters are called \textit{linear models}.

We can extend the class of models considering linear combinations of nonlinear functions of the input variables, i.e.

\begin{equation}
   y(x,\mathbf{w}) = \sum_{j=0}^{M-1} \phi_j (x) w_j 
   \label{eq:lin-reg-model-phi}
\end{equation}

where $\phi_j (x)$ are known as \textit{basis functions}, and then the total number of parameters for this model will be $M$. In the example of the curve fitting, the polynomial regression implies that $\phi_j(x)=x^j$. We can evaluate the same operation of (\ref{eq:lin-reg-model-1}) in the matrix form by

\begin{equation}
   y(x,\mathbf{w}) = \boldsymbol{\phi}(x)^\top  \mathbf{w}
   \label{eq:lin-reg-model-1-matrix-form}
\end{equation}

where $\boldsymbol{\phi}(x) = \left[ \phi_0(x), \dots, \phi_{M-1}(x) \right]^\top$. 

\begin{figure}[htbp]
   \centering
   \includegraphics{graphics/prml/{Figure1.3}.pdf}
   \caption{The error function (\ref{eq:lin-reg-error-function-1}) corresponds to (one half of) the sum of the squares of the displacements (shown by the vertical green bars) of each data point from the function $y(x, \mathbf{w})$ \cite{Bishop:2006:PRM:1162264}.}
   % \label{<label>}
\end{figure}

The values of $\mathbf{w}$ are obtained by minimizing the \textit{error function}, a measure of the distance between the training data set and $y$, given values of $\mathbf{w}$. By the way, the chosen error function will be 

\begin{equation}
   \label{eq:lin-reg-error-function-1}
   E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\{ y(x_i,\mathbf{w})-y_i \right\}^2
\end{equation}

This indicate that if $E$ is zero, $y$ passes exactly through each training data point. Observe that $E$ do not assume negative values because of its quadratic form, then we can find $\mathbf{w}$ by finding the minimum value of $E$, denoted $\mathbf{w^*}$, by

\begin{equation}
   \frac{\partial E}{\partial \mathbf{w}} = 0
\end{equation}

We can rewrite (\ref{eq:lin-reg-error-function-1}) in the matrix form, as $\mathbf{y} = y(\mathbf{x},\mathbf{w})$, where $\mathbf{x}=\left[ x_0,\dots, x_{N-1} \right]^\top$, i.e. $y$ evaluated for all input variables (See Appendix \ref{subsec:app-matrix-form}). Then we have

\begin{equation}
   \label{eq:lin-reg-error-function-matrix-form}
   \mathbf{y} = \Phi \mathbf{w}
\end{equation}

where $\Phi$ is the \textit{design matrix} such that $\boldsymbol{\phi}(x)$ is evaluated for all $\mathbf{x}$. Proceeding with the minimization, we obtain the optimal $\mathbf{w}$, or $\mathbf{w}^*$ by

\begin{equation}
   \label{eq:lin-reg-opt-param}
   \mathbf{w}^{*} = (\Phi^\top \Phi)^{-1}\Phi^\top \mathbf{t}
\end{equation}

which are the parameters that best fit the model to the data.

\begin{figure}
   \centering
   \begin{subfigure}[htpb]{0.45\linewidth}
       \centering
       \includegraphics[width=1\linewidth]{graphics/prml/{Figure1.4a}.pdf}
   \end{subfigure}
   \hfill
   \begin{subfigure}[htpb]{0.45\linewidth}
       \centering
       \includegraphics[width=1\linewidth]{graphics/prml/{Figure1.4b}.pdf}       
   \end{subfigure}
   \hfill \\
   \begin{subfigure}[htpb]{0.45\linewidth}
       \centering
       \includegraphics[width=1\linewidth]{graphics/prml/{Figure1.4c}.pdf}
   \end{subfigure}
   \hfill 
   \begin{subfigure}[htpb]{0.45\linewidth}
       \centering
       \includegraphics[width=1\linewidth]{graphics/prml/{Figure1.4d}.pdf}
   \end{subfigure}
      \caption{Plots of polynomials for the model in (\ref{eq:lin-reg-model-1}) having various orders $M$, shown as red curves \cite{Bishop:2006:PRM:1162264}.}
      \label{fig:lin-reg-overfit}
\end{figure}

As we increase the number of parameters, our model becomes more flexible and then our error function approximates of zero for the training data. But when compared to the test data, the error increases. This is known as \textit{over-fitting} as seen in Figure \ref{fig:lin-reg-overfit} for $M=9$.

\subsection{Regularized Linear Regression}

\begin{table}[htpb]
   \centering
   \begin{tabular}{l| l l l l}
     & $M=0$ & $M=1$ & $M=6$ & $M=9$ \\
   \hline
   $\mathbf{w}_0^*$ & 0.19 & 0.82   & 0.31   & 0.35\\
   $\mathbf{w}_1^*$ &      & -1.27  & 7.99   & 232.37\\
   $\mathbf{w}_2^*$ &      &        & -25.43 & -5321.83\\
   $\mathbf{w}_3^*$ &      &        & 17.37  & 48568.31\\
   $\mathbf{w}_4^*$ &      &        &        & -231639.30\\
   $\mathbf{w}_5^*$ &      &        &        & 640042.26\\
   $\mathbf{w}_6^*$ &      &        &        & -1061800.52\\
   $\mathbf{w}_7^*$ &      &        &        & 1042400.18\\
   $\mathbf{w}_8^*$ &      &        &        & -557682.99\\
   $\mathbf{w}_9^*$ &      &        &        & 125201.43\\
   \end{tabular}
   \caption{Table of the coefficients $\mathbf{w} ^*$ for polynomials of various order. Observe how the typical magnitude of the coefficients increases dramatically as the order of the polynomial increases \cite{Bishop:2006:PRM:1162264}.}
   \label{tab:reg-lin-reg-weights}
   \end{table}

An approach to controls the over-fitting problem is to deal with the flexibility of the model. In the Table \ref{tab:reg-lin-reg-weights} we can note that, for a larger number of parameters, the derivative of the function $y$ takes larger constants. Then to control the over-fitting, we can be done by controlling the norm of $\mathbf{w}^*$ as the number of parameters increases. By (\ref{eq:lin-reg-error-function-1}) we can add the penalty term $||\mathbf{w}||^2 $ scaled by the factor $\lambda / 2$, then


\begin{subequations}
   \begin{equation}
      \label{eq:lin-reg-error-function-regularized}
      E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\{ y(x_i,\mathbf{w})-y_i \right\}^2 + \frac{\lambda}{2} ||\mathbf{w}||^2
   \end{equation}
   \begin{equation}
      \label{eq:lin-reg-error-function-regularized-matrix-form}
      \mathbf{w}^{*} = (\Phi^\top \Phi + \lambda \mathbf{I})^{-1}\Phi^\top \mathbf{t} 
   \end{equation}
\end{subequations}

whats means that our error increases as the norm of $\mathbf{w}$ grows, ans we can control this increasing by adjusting the term $\lambda$ as we want. By this we see in \autoref{fig:reg-lin-reg-overfit-lambda} that the regularization acts like a \textit{smoothing} factor over the function.


% This allow us to increase the number of parameters trying to control the over-fitting. More, add parameters will allows us to capture different aspects of the data set, what we will see later.

\begin{figure}[htpb]
   \centering
   \begin{subfigure}[htpb]{0.45\linewidth}
       \centering
       \includegraphics[width=1\linewidth]{graphics/prml/{Figure1.7a}.pdf}
   \end{subfigure}
   \hfill
   \begin{subfigure}[htpb]{0.45\linewidth}
       \centering
       \includegraphics[width=1\linewidth]{graphics/prml/{Figure1.7b}.pdf}       
   \end{subfigure}
      \caption{Plots of $M = 9$ polynomials fitted to the data set using the regularized error function (\ref{eq:lin-reg-error-function-regularized}) for two values of the regularization parameter $\lambda$ corresponding to $\ln \lambda = -18$ and $\ln \lambda = 0$. The case of no regularizer, i.e., $\lambda = 0$, corresponding to $\ln \lambda = -\infty$, is shown at the bottom right of \autoref{fig:lin-reg-overfit} \cite{Bishop:2006:PRM:1162264}.}
      \label{fig:reg-lin-reg-overfit-lambda}
\end{figure}

\section{Bayesian Linear Regression}

\subsection{A Bayesian view of Linear Regression}

Until now, we see the curve fitting problem in terms of the minimization of the error function. Then we will see the same by a probabilistic perspective gaining some insights into error minimization and regularization, leading us to a full Bayesian treatment.

\begin{equation}
   \label{eq:bay-reg-bayes-rule}
   p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}
\end{equation}

We can use the Bayes' theorem (\ref{eq:bay-reg-bayes-rule}) to convert a \textit{prior} probability into a \textit{posterior} probability at the light of some evidence. We can make assumptions about quantities such as the parameters $\mathbf{w}$ in the form of a prior distribution $p(\mathbf{w})$. The observation of the data $\mathcal{D}$ and what its implies in the parameters is expressed as a conditional probability $p(\mathcal{D}|\mathbf{w})$. Then we can evaluate the uncertainty about $\mathbf{w}$ after observed the data $\mathcal{D}$ as a posterior probability $p(\mathbf{w}|\mathcal{D})$.

The quantity $p(\mathcal{D}|\mathbf{w})$ expresses how probable the observed data $\mathcal{D}$ is for different settings of $\mathbf{w}$. Then, not being necessarily a probability distribution, but a function over the parameters \cite{degroot2012probability}, its integral with respect to $\mathbf{w}$ could not be equal one, then to normalize the equation with respect to the left-side there's a term $p(\mathcal{D})$. This distribution is called \textit{likelihood function}.

Integrating the both sides with respect to $\mathbf{w}$, we obtain the denominator, then considering that integrating a probability distribution over itself is equal to one, we have

\begin{equation}
   \label{eq:bay-lin-bayes-theorem}
   p(\mathcal{D})=\int p(\mathcal{D} | \mathbf{w}) p(\mathbf{w}) \mathrm{d} \mathbf{w}
\end{equation}

\subsection{Bayesian curve fitting}


Let's consider the same data set $\mathcal{D}$ presented before, but now we have some uncertainty over the value of the measured value $t$. This uncertainty can be represented as a probability distribution function $p$, in this particular case a Gaussian distribution, with a mean equal to the model $y(x,\mathbf{w})$. Thus we have

\begin{equation}
   \label{eq:bay-cur-fit-1}
   p(t | x, \mathbf{w}, \beta)=\mathcal{N}\left(t | y(x, \mathbf{w}), \beta^{-1}\right)
\end{equation}


\begin{figure}[htpb]
   \centering
   \includegraphics{graphics/prml/{Figure1.16}}
   \caption{Schematic illustration of a Gaussian conditional distribution for $t$ given $x$ given by  (\ref{eq:bay-cur-fit-1}), in which the mean is given by the polynomial function y(x, w), and the precision is given by the parameter $\beta$, which is related to the variance $\beta^{-1}=\sigma^2$ \cite{Bishop:2006:PRM:1162264}.}
   % \label{<label>}
\end{figure}

Where $\beta$ is the variance of the distribution. Note that a large $\beta$ will give us more imprecision about the measured value $t$, then we can call it of \textit{precision parameter}, i.e. how much certain we are about $t$. As we done in linear regression, we are trying to obtain the parameters for the model. In other words, given a value $t$, we trying to obtain the \textit{mean} and the \textit{variance} which maximize the probability of the measured value. Assuming the data set being independent and identically distributed, the joint probability of the whole data set will be

\begin{equation}
   p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \beta)= \prod_{i=0}^{N-1} \mathcal{N}\left(t_i | y(x_i, \mathbf{w}), \beta^{-1}\right)
\end{equation}

When viewed as function of $y(x_i, \mathbf{w})$, the model, and $\beta^{-1}$, this is the likelihood function for the Gaussian. The parameters of the distribution can be determined by maximizing the likelihood function. It is convenient to maximize the log of the likelihood function, or minimize the negative log whats is equivalent, this implies that the maximization of the log of the function is equivalent to the maximization of the function itself, because the logarithm is a monotonically increasing of its argument. This helps the mathematical analysis and helps numerically because the small probabilities can easily underflow the numerical precision of the computer. Then\footnote[2]{Consider the Gaussian distribution as $\mathcal{N}\left(x | \mu, \sigma^{2}\right)=\left(2 \pi \sigma^{2}\right)^{-1 / 2} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}$}

\begin{equation}
   \label{eq:bay-reg-ml-function}
   \ln p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \beta)=-\frac{\beta}{2} \sum_{i=1}^{N}\left\{y\left(x_{i}, \mathbf{w}\right)-t_{i}\right\}^{2}+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)
\end{equation}

The maximization of (\ref{eq:bay-reg-ml-function}) taking the derivative with respect to $\mathbf{w}$ will lead us back to 
the same of the minimization of (\ref{eq:lin-reg-error-function-1}), the error function of the linear regression. Here, just by notation, we will call the resulting parameters of the maximization of $\mathbf{w}_{\text{ML}}$, what it is called \textit{maximum likelihood}.

We can determine the precision parameter using the maximum likelihood by taking the derivative with respect to $\beta$ of (\ref{eq:bay-reg-ml-function}), what gives

\begin{equation}
   \frac{1}{\beta_{\mathrm{ML}}}=\frac{1}{N} \sum_{i=0}^{N-1}\left\{y\left(x_{i}, \mathbf{w}_{\mathrm{ML}}\right)-t_{i}\right\}^{2}
\end{equation}

Now we have a probabilistic view of the regression and then we can make predictions for new values of $x$, given that our model is capable of learn the parameters. And not just one collection of them, but a distribution probability.

In other words, after find the maximum likelihood parameters $\mathbf{w}_\text{ML}$ and $\beta_\text{ML}$, we have the parameters distribution by

\begin{equation}
   p\left(t | x, \mathbf{w}_{\mathrm{ML}}, \beta_{\mathrm{ML}}\right)=\mathcal{N}\left(t | y\left(x, \mathbf{w}_{\mathrm{ML}}\right), \beta_{\mathrm{ML}}^{-1}\right)
\end{equation}

Aiming to apply a "more Bayesian" approach, we not have yet a prior distribution to make the inference using the Bayes' rule. We can now introduce here the probability distribution over the parameters $p(\mathbf{w})$ as presented in the Section 3.1. The choice is arbitrary, but for this particular case we will consider

\begin{equation}
   \label{eq:bay-lin-reg-prior-dist}
   p(\mathbf{w} | \alpha)=\mathcal{N}\left(\mathbf{w} | \mathbf{0}, \alpha^{-1} \mathbf{I}\right)=\left(\frac{\alpha}{2 \pi}\right)^{(M+1) / 2} \exp \left\{-\frac{\alpha}{2} \mathbf{w}^\top \mathbf{w}\right\}
\end{equation}

where $\alpha$ is the variance, or precision parameter, of the distribution and $M+1$ is the number of parameters of the model, i.e. the length of $\mathbf{w}$. We call \textit{hyperparameters} the variables such $\alpha$ who control the model parameters distribution. And now we have by the Bayes' theorem considering that our \textit{posterior} distribution is proportional to the product between the \textit{likelihood function} and the assumed \textit{prior}, as seen in (\ref{eq:bay-lin-bayes-theorem}) then, assuming the observation of the whole data set

\begin{equation}
   p(\mathbf{w} | \mathbf{x}, \mathbf{t}, \alpha, \beta) \propto p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w} | \alpha)
\end{equation}

As done before, we maximize the posterior probability, i.e. find the most probable value given the data by the term $p(\mathbf{w} | \mathbf{x}, \mathbf{t}) $ aside of the distribution parameters. This will result a particular choice of $\mathbf{w}$. We call this approach of \textit{maximum posterior}, or MAP. Then taking the negative logarithm
%
\begin{equation}
   \ln p(\mathbf{w} | \mathbf{x}, \mathbf{t}, \alpha, \beta) \propto \ln p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \beta) + \ln p(\mathbf{w} | \alpha)
\end{equation}
%
Then we substitute the probability distributions founded before. Note that the first term in the right side is the error function founded in (\ref{eq:bay-reg-ml-function}). Then, the terms which the minimization depends of $\mathbf{w}$ are

\begin{equation}
    \frac{\beta}{2} \sum_{i=1}^{N}\left\{y\left(x_{i}, \mathbf{w}\right)-t_{i}\right\}^{2} + \frac{\alpha}{2} \mathbf{w}^\top \mathbf{w}
\end{equation}

And we can note the similarity with the regularized linear regression in (\ref{eq:lin-reg-error-function-regularized}) aside of the term $\lambda$, what can be founded by $\lambda=\alpha / \beta$. It's important to note that even called maximum posterior, here it was presented a minimization in terms of the negative logarithm, but this equals to the maximization of the positive logarithm. The signal was chosen just for similarity with the error function.

\subsection{Bayesian inference}

The similarity between the maximization and the Bayesian approach mentioned before shows that the second comprise even a model training such as the classical regression, as also the control of the over fitting by the regularization. But to say that our model is in fact Bayesian, we might obtain not just a single value, as in MAP, but its distribution. This requires the application the fully Bayes' theorem as (\ref{eq:bay-reg-bayes-rule}). Then, we have

\begin{equation}
   \overbrace{p(\mathbf{w}|\mathbf{t})}^{posterior} = \frac{p(\mathbf{t}|\mathbf{w}) p(\mathbf{w})}{p(\mathbf{t})} = \frac{\overbrace{p(\mathbf{t}|\mathbf{w})}^{\text{\textit{likelihood}}} \overbrace{p(\mathbf{w})}^{\text{\textit{prior}}}}{\underbrace{\int p(\mathbf{t}|\mathbf{w}) p(\mathbf{w}) d\mathbf{w}}_{\text{\textit{marginal distribution}}}}
\end{equation}

This is called \textit{Bayesian inference}. If we assume that all distributions which we are working are Gaussian, the posterior distribution has closed form. To do that, we make use of the closure under linear transformations, or \textit{affine transformations}, for the Gaussian. For that, we will make use of the corollary below, which theorems are proven in the \autoref{subsec:app-par-gau}.

\begin{corollary}

   Being $\mathbf{x}_b$ conditioned on $\mathbf{x}_a$ and Gaussian distributed as

   \begin{equation}
     p\left(\mathbf{x}_{a}\right)=\mathcal{N}\left(\mathbf{x}_{a} | \boldsymbol{\mu}_{a}, \boldsymbol{\Sigma}_{a}\right), \quad p\left(\mathbf{x}_{b} | \mathbf{x}_{a}\right)=\mathcal{N}\left(\mathbf{x}_{b} | \mathbf{M} \mathbf{x}_{a}+\mathbf{d}, \boldsymbol{\Sigma}_{b | a}\right)
   \end{equation}

   with $\boldsymbol{\mu}_{b} =\mathbf{M} \boldsymbol{\mu}_{a}+\mathbf{d}$, $\boldsymbol{\Sigma}_{b} =\boldsymbol{\Sigma}_{b | a}+\mathbf{M} \boldsymbol{\Sigma}_{a} \mathbf{M}^\top$, $\mathbf{M}$ a constant matrix and $\mathbf{d}$ a constant vector, both with the appropriate dimensions. Then conditional distribution $p(\mathbf{x}_a|\mathbf{x}_b)$ is given by

   \begin{subequations}
   
   \begin{align}
     p\left(\mathbf{x}_{a} | \mathbf{x}_{b}\right)&=\mathcal{N}\left(\mathbf{x}_{a} | \boldsymbol{\mu}_{a | b}, \boldsymbol{\Sigma}_{a | b}\right)
   \end{align}
   with
   \begin{align}
       \boldsymbol{\mu}_{a | b}&=\boldsymbol{\Sigma}_{a | b}\left(\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1}\left(\mathbf{x}_{b}-\mathbf{d}\right)+\boldsymbol{\Sigma}_{a}^{-1} \boldsymbol{\mu}_{a}\right) \\ \boldsymbol{\Sigma}_{a | b}&=\left(\boldsymbol{\Sigma}_{a}^{-1}+\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}\right)^{-1}.
   \end{align}
 \end{subequations}
 \end{corollary}

 \begin{figure}[htpb]
   \centering
   \includegraphics[width=.95\linewidth]{graphics/prml/{Figure3.7}}
   \caption{Illustration of sequential Bayesian learning for a simple linear model of the form $y(x, \mathbf{w}) = w_0 + w_1 x$. The hyperparameters $\alpha$ and $\beta$ are assumed as $2$ and $25$, respectively, just by example \cite{Bishop:2006:PRM:1162264}.}
   % \label{<label>}
\end{figure}

 Assuming no deviation in the mean, $\mathbf{d} = \mathbf{0}$, and the linear transformation being our design matrix, $\mathbf{M} = \Phi$, we obtain that

 \begin{equation}
   \label{eq:bay-inf-posterior}
    p(\mathbf{w}|\mathbf{t},\alpha,\beta) = \mathcal{N} \left( \mathbf{w} | \boldsymbol{\mu}_{\mathbf{w}|\mathbf{t}} , \boldsymbol{\Sigma}_{\mathbf{w}|\mathbf{t}}\right)
 \end{equation}

 being

 \begin{equation}
   \boldsymbol{\mu}_{\mathbf{w}|\mathbf{t}}=\boldsymbol{\Sigma}_{\mathbf{w}|\mathbf{t}}\left(\beta \Phi^\top \mathbf{t}+\boldsymbol{\Sigma}_{\mathbf{w}}^{-1} \boldsymbol{\mu}_{\mathbf{w}}\right), \quad \boldsymbol{\Sigma}_{\mathbf{w}|\mathbf{t}}=\left(\boldsymbol{\Sigma}_{\mathbf{w}}^{-1}+ \beta \Phi^\top \Phi\right)^{-1}
 \end{equation}

 assumed the prior distribution defined in (\ref{eq:bay-lin-reg-prior-dist}) and the precision matrix $\boldsymbol{\Sigma}_{\mathbf{t}|\mathbf{w}} = \beta^{-1}\mathbf{I}$. Then we have defined

 \begin{equation}
    \boldsymbol{\mu}_\mathbf{w}=\mathbf{0}, \quad \boldsymbol{\Sigma}_\mathbf{w} = \alpha^{-1} \mathbf{I}
 \end{equation}

\subsection{Predictive distribution}

In practice, some times it is more valuable the information about $t$ itself than its parameters $\mathbf{w}$. We can make this by evaluating the predictions of $t$ for the new values of $x$ by

\begin{equation}
   p(t | \mathbf{t}, \alpha, \beta)=\int p(t | \mathbf{w}, \beta) p(\mathbf{w} | \mathbf{t}, \alpha, \beta) \mathrm{d} \mathbf{w}
\end{equation}

what is called \textit{predictive distribution}. The distributions under integration were defined in (\ref{eq:bay-cur-fit-1}) and (\ref{eq:bay-inf-posterior}). Then we use the \textit{marginalization} defined in \autoref{subsec:app-par-gau} and obtain that

\begin{equation}
   p(t | \mathbf{x}, \mathbf{t}, \alpha, \beta)=\mathcal{N}\left(t | \boldsymbol{\mu}_{t}, \boldsymbol{\Sigma}_{t} \right)
\end{equation}

with $\boldsymbol{\mu}_y = \boldsymbol{\phi}(x)^\top \boldsymbol{\Sigma}_{\mathbf{w} | \mathbf{t}}$ and $\boldsymbol{\Sigma}_y = \beta^{-1} + \boldsymbol{\phi}(x)^\top \boldsymbol{\Sigma}_{\mathbf{w} | \mathbf{t}}\boldsymbol{\phi}(x)$. Note that we are considering the linear model as $y(x,\mathbf{w}) = \boldsymbol{\phi}(x)^\top \mathbf{w}$, i.e. the affine transformation $\mathbf{M}$ here is $\boldsymbol{\phi}(x)^\top$.

An alternative formulation \cite{Rasmussen:2005:GPM:1162254} is

\begin{equation}
\begin{aligned}
      p(t | \mathbf{x}, \mathbf{t}, \alpha, \beta) = \mathcal{N} & \left(  \boldsymbol{\phi}(x)^{\top} \Sigma_\mathbf{w} \Phi\left(K+\beta I\right)^{-1} \mathbf{t} \right., \\ &\left.\boldsymbol{\phi}(x)^{\top} \Sigma_\mathbf{w} \boldsymbol{\phi}(x)-\boldsymbol{\phi}(x)^{\top} \Sigma_\mathbf{w} \Phi\left(K+\beta I\right)^{-1} \Phi^{\top} \Sigma_\mathbf{w} \boldsymbol{\phi}(x)\right)
\end{aligned}
\end{equation}

with $K=\Phi^{\top} \boldsymbol{\Sigma}_\mathbf{w} \Phi$.

\subsection{Kernels}

In the linear regression, in particular particular, we fit the data using a polynomial function of the form
%
\begin{equation}
      f(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}
      \label{eq:intro-poly}
\end{equation}
%
where $M$ is the \textit{order} of the polynomial, and $x^j$ denotes $x$ raised to the power of $j$. The polynomial coefficients $w_0, \dots , w_M$ are collectively denoted by the vector $\mathbf{w}$. Note that, although the polynomial function $f(x, \mathbf{w})$ is a nonlinear function of $x$, it is a linear function of the coefficients $w$. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called \textit{linear models}. In general, we could write this weighted sum with any other function. In other words, we can put this in terms of $\phi_n(x)=x^n$, where $\phi$ could be other \textit{basis function}, e.g. we could have different functions $f$ for different basis functions.
%
\begin{align*}
      f(x,\mathbf{w}) &= w_0 \phi_0(x) +w_1 \phi_1(x) +w_2 \phi_2(x)  + ... + w_{M-1} \phi_{M-1}(x); \\
                        &= w_0 \exp\left\{ - \frac{(x-\mu_0)^2}{2\sigma^2}\right\} + w_1  \exp\left\{ - \frac{(x-\mu_1)^2}{2\sigma^2}\right\} + \\ & ... + w_{M-1} \exp\left\{ - \frac{(x-\mu_{M-1})^2}{2\sigma^2}\right\}; \\
                        &= w_0 \sin(0 \cdot x) + w_1 \cos(1 \cdot x) + \\ &... + w_{M-2} \sin((M-2) \cdot x) + w_{M-1} \cos((M-1) \cdot x); \\
                        &=\sum_{j=0}^{M-1} w_{j} \phi_{j}(x);
\end{align*}
%

With this, our linear model is able to capture different aspects of the data set, as periodicity, exponential increasing, roughness etc. When the number of basis functions tends to infinite these models become \textit{kernels} \cite{mackay1998introduction}. It's important to note that these linear models are needed to define its basis functions before the training data set is observed.

Notice that in the previous section, we are using transformations always of the type $\Phi^{\top} \boldsymbol{\Sigma}_\mathbf{w} \Phi$, $\boldsymbol{\phi}(x)^{\top} \boldsymbol{\Sigma}_\mathbf{w} \Phi$, or $\boldsymbol{\phi}(x)^{\top} \boldsymbol{\Sigma}_\mathbf{w} \boldsymbol{\phi}(x)$. Then we can generalize the form $\boldsymbol{\phi}(\mathbf{x})^\top \boldsymbol{\Sigma}_\mathbf{w}\boldsymbol{\phi}(\mathbf{x'})$ where $\mathbf{x}$ and $\mathbf{x'}$ are in either the training or the test sets. We define this form as $k(\mathbf{x},\mathbf{x'})$, where $k(\cdot,\cdot)$ is called \textit{covariance function}, or \textit{kernel}. Further insight into the role of the equivalent kernel can be obtained by considering the covariance between $y(\mathbf{x})$ and $y(\mathbf{x'})$, which is given by

\begin{equation}
   \begin{aligned}
      \mathrm{cov}\left\{ y(\mathbf{x}), y(\mathbf{x'})\right\} &=\mathrm{cov} \left\{ \boldsymbol{\phi}(\mathbf{x})^\top \mathbf{w}, \mathbf{w}^\top \boldsymbol{\phi}(\mathbf{x}) \right\} \\
       &= \boldsymbol{\phi}(\mathbf{x})^\top \boldsymbol{\Sigma}_\mathbf{w}\boldsymbol{\phi}(\mathbf{x}) = \beta^{-1}k(\mathbf{x},\mathbf{x'})
   \end{aligned}
\end{equation}

From the form of the kernel, we see that the predictive mean at nearby points will be highly correlated, whereas for more distant pairs of points the correlation will be smaller. The linearity preserves the propriety of addition, then our model accepts the junction of others functions to create a new kernel, in order to capture some aspects of the data set. 

\section{Gaussian processes}

Until now we have made the inference in the \textit{feature space}, the space where the parameters $\mathbf{w}$ are. In other words, the strategy was to train our model, obtaining the parameters probability distribution, by Bayesian inference, and then evaluating the predictive distribution with the posterior of the inference.

An alternative and equivalent way to achieve such results is to make the inference directly in the space of functions, or \textit{function space}. To this we use the \textit{Gaussian processes} to describe the distribution over the functions directly \cite{Rasmussen:2005:GPM:1162254}.

We define a Gaussian process (GP) as a collection of random variables, such that any finite number of which is normal jointly distributed. In other words, it can be thought  of as a generalization of a Gaussian distribution over a finite vector space to a function space of infinite dimension \cite{mackay1998introduction}. As the normal distribution, the GP is completely defined by its mean function $m(\mathbf{x})$ and covariance function $k(\mathbf{x},\mathbf{x'})$ of a real process $y(\mathbf{x})$, these in turn are defined as

\begin{equation}
   m(\mathbf{x}) = \mathbb{E}\left\{ y(\mathbf{x}) \right\}, \quad
   k(\mathbf{x},\mathbf{x'}) = \mathbb{E}\left\{ \left( y(\mathbf{x}) - m(\mathbf{x}) \right) \left( y(\mathbf{x'}) - m(\mathbf{x'}) \right) \right\}
\end{equation}

and finally

\begin{equation}
   y(\mathbf{x}) \sim \mathcal{GP} \left( m(\mathbf{x}), k(\mathbf{x},\mathbf{x'}) \right)
\end{equation}

Such collection definition automatically implies in the marginalization property already present for the multidimensional Gaussian distributions. For the GP this means that the observation of a larger set of variables does not change the distribution of the smaller set.

This is important to obtain our Bayesian linear regression as a GP. Being our model $y(\mathbf{x})=\boldsymbol{\phi}(\mathbf{x})^\top \mathbf{w}$ with prior $\mathcal{N}\left( \mathbf{w} | \mathbf{0}, \boldsymbol{\Sigma}_{\mathbf{w}}\right)$. We obtain for the mean and covariance functions

\begin{equation}
   \label{eq:gau-pro-mean-cov}
   \begin{aligned}
      \mathbb{E}\left\{ y(\mathbf{x}) \right\} &= \boldsymbol{\phi}(\mathbf{x})^\top \mathbb{E} \left\{ \mathbf{w} \right\} = 0,\\
      \mathbb{E}\left\{ y(\mathbf{x})y(\mathbf{x'}) \right\} &= \boldsymbol{\phi}(\mathbf{x})^\top \mathbb{E} \left\{ \mathbf{ww}^\top \right\}\boldsymbol{\phi}(\mathbf{x}) = \boldsymbol{\phi}(\mathbf{x})^\top \boldsymbol{\Sigma}_{\mathbf{w}} \boldsymbol{\phi}(\mathbf{x'})
   \end{aligned}
\end{equation}

\subsection{Prediction with Noisy Observations}

The last sections give us an insight about the construction of the learning process for the GP. Analogous to the Bayes' theorem for the Gaussian, we use the idea of the GP as a multidimensional Gaussian distribution and we can make inference with its partitions. 

First, to take the concept, we will consider the case where the observations are noise free. We will substitute the covariance matrices from the partitioned Gaussian distributions by the covariance function applied at the points of the observations $\mathbf{t}$ and the prediction $\mathbf{t_*}$, as

\begin{equation}
   \left( \begin{array}{l}{\mathbf{t}}  \\ {\mathbf{t}_*} \end{array} \right) 
   \sim
   \mathcal{N} \left( \mathbf{0}, \left[ \begin{array}{ll}{\mathbf{K}(\mathbf{x},\mathbf{x})} & {\mathbf{K}(\mathbf{x},\mathbf{x_*})} \\ {\mathbf{K}(\mathbf{x_*},\mathbf{x})} & {\mathbf{K}(\mathbf{x_*},\mathbf{x_*})}\end{array} \right] \right)
\end{equation}

where $\mathbf{K}(\cdot,\cdot)$ denotes the covariance matrices evaluated at all pairs of $N$-dimensional $\mathbf{x}$ training points and $N_*$-dimensional $\mathbf{x_*}$ test points. Then, making use of \autoref{subsec:app-par-gau}, we use the \textit{conditioning} to obtain the predictive distribution

\begin{equation}
\begin{aligned}
      p(\mathbf{y_*}|\mathbf{x_*},\mathbf{x},\mathbf{y}) = \mathcal{N} & \left( \mathbf{y_*} | \mathbf{K}(\mathbf{x_*},\mathbf{x})\mathbf{K}(\mathbf{x},\mathbf{x})^{-1}\mathbf{y}, \right. \\ & \left. \mathbf{K}(\mathbf{x_*},\mathbf{x_*})-\mathbf{K}(\mathbf{x_*},\mathbf{x})\mathbf{K}(\mathbf{x},\mathbf{x})^{-1}\mathbf{K}(\mathbf{x},\mathbf{x_*}) \right)
\end{aligned}
\end{equation}

Now assuming the noise in the observations, what is a more realistic modelling situation, we have that $t = y(\mathbf{x}) + \varepsilon$, being $\varepsilon$ the Gaussian noise with variance $\beta^{-1}$. Then we have that $\mathrm{cov}(\mathbf{t})=\mathbf{K}(\mathbf{x},\mathbf{x}) + \beta^{-1}\mathbf{I}$.

Deriving the conditional distribution corresponding we arrive at the key predictive equations for Gaussian process regression

\begin{equation}
\begin{aligned} p(\mathbf{y}_{*} | \mathbf{x}, \mathbf{t},\mathbf {x_{*}}) &= \mathcal{N}\left(\overline{\mathbf{y}}_{*}, \mathrm{cov}\left(\mathbf{y}_{*}\right)\right), \text { where } \\ \overline{\mathbf{y}}_{*} & \triangleq \mathbb{E}\left\{ \mathbf{y}_{*} | \mathbf{x}, \mathbf{t}, \mathbf{x_{*}}\right\}=\mathbf{K}\left(\mathbf{x_{*}}, \mathbf{x}\right)\left(\mathbf{K}(\mathbf{x}, \mathbf{x})+\beta^{-1} \mathbf{I}\right)^{-1} \mathbf{t} \\ \mathrm{cov}\left(\mathbf{y}_{*}\right) &=\mathbf{K}\left(\mathbf{x_{*}}, \mathbf{x_{*}}\right)-\mathbf{K}\left(\mathbf{x_{*}}, \mathbf{x}\right)\left(\mathbf{K}(\mathbf{x}, \mathbf{x})+\beta^{-1} \mathbf{I}\right)^{-1} \mathbf{K}\left(\mathbf{x}, \mathbf{x_{*}}\right) \end{aligned}
\end{equation}

\begin{figure}[htpb]
   \centering
   \includegraphics{graphics/prml/{Figure6.8}}
   \caption{Illustration of Gaussian process regression applied to the sinusoidal data set in which the three right-most data points have been omitted. The green curve shows the sinusoidal function from which the data points, shown in blue, are obtained by sampling and addition of Gaussian noise. The red line shows the mean of the Gaussian process predictive distribution, and the shaded region corresponds to plus and minus two standard deviations. Notice how the uncertainty increases in the region to the right of the data points \cite{Bishop:2006:PRM:1162264}.}
   % \label{<label>}
\end{figure}

\section{Hyperparameters}

As in the distributions, the kernels have its parameters to be set before the model training. Such parameters, called hyperparameters can assume a distribution over itself, and with this, we can learn the parameters to the data just as we've been doing for the inference. In general, the posterior distribution for the hyperparameters are intractable, then we cal use MAP to choose at least one value (the most probable one), or evaluate the integral numerically.

\newpage
\input{Intro_to_GP_appendices}

\newpage
\bibliography{Intro_to_GP_bibliography}

\end{document}