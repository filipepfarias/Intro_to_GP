\documentclass{article} % For LaTeX2e
\usepackage{theme}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

%%% Bibliography
\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}


\title{Introduction to Gaussian Processes}


\author{
Filipe P. de Farias
% \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.}
\\
Teleinformatics Engineering Department\\
Federal University of Cear√°\\
Fortaleza, CE - Brazil\\
\texttt{filipepfarias@fisica.ufc.br} \\
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% (if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
   Roughly speaking a stochastic process is a generalization of a probability distribution (which describes a finite-dimensional random variable) to \textit{functions}. By focussing on processes which are \textit{Gaussian}, it turns out that the computations required for inference and learning become relatively easy. Thus, the supervised learning problems in machine learning which can be thought of as learning a function from examples can be cast directly into the Gaussian process framework.\cite{rasmussen2006gaussian}
\end{abstract}

\section{Introduction}

The problem of searching for patterns in data is a fundamental one and has a long and successful history. The discovery of regularities in atomic spectra played a key role in the development and verification of quantum physics in the early twentieth century. The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.

Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as \textit{supervised learning} problems. If the desired output consists of one or more continuous variables, then the task is called \textit{regression}. An example of a regression problem would be the prediction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations of reactants, the temperature, and the pressure.\cite{bishop2006pattern}

In general we denote the input as $\mathbf{x}$, and the output (or target) as $y$. The input is usually represented as a vector $\mathbf{x}$ as there are in general many input variables. We have a dataset $\mathcal{D}$ of $n$ observations, $\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right) | i=1, \ldots, n\right\}$. Given this training data we wish to make predictions for new inputs $\mathbf{x^*}$ that we have not seen in the \textit{training set}. Thus it is clear that the problem at hand is inductive; we need to move from the finite training data $\mathcal{D}$ to a function $f$ that makes predictions for all possible input values. To do this we must make assumptions about the characteristics of the underlying function, as otherwise any function which is consistent with the training data would be equally valid.

A wide variety of methods have been proposed to deal with the \textit{supervised learning} problem; here we describe two common approaches. The first is to restrict the class of functions that we consider, for example by only considering linear functions of the input. The second approach is (speaking rather loosely) to give a prior probability to every possible function, where higher probabilities are given to functions that we consider to be \textit{more likely}, for example because they are smoother than other functions.

The first approach has an obvious problem in that we have to decide upon the richness of the class of functions considered; if we are using a model based on a certain class of functions (e.g. linear functions) and the target function is not well modelled by this class, then the predictions will be poor. One may be tempted to increase the flexibility of the class of functions, but this runs into the danger of \textit{overfitting}, where we can obtain a good fit to the training data, but perform badly when making test predictions.

The second approach appears to have a serious problem, in that surely there are an uncountably infinite set of possible functions, and how are we going to compute with this set in finite time? This is where the Gaussian \textit{process} comes to our rescue. A Gaussian process is a generalization of the Gaussian probability \textit{distribution}. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic \textit{process} governs the properties of functions. Leaving mathematical sophistication aside, one can loosely think of a function as a very long vector, each entry in the vector specifying the function value $f(x)$ at a particular input $x$. It turns out, that although this idea is a little naive, it is surprisingly close what we need. Indeed, the question of how we deal computationally with these infinite dimensional objects has the most pleasant resolution imaginable: if you ask only for the properties of the function at a finite number of points, then inference in the Gaussian process will give you the same answer if you ignore the infinitely many other points, as if you would have taken them all into account! And these answers are consistent with answers to any other finite queries you may have. One of the main attractions of the Gaussian process framework is precisely that it unites a sophisticated and consistent view with computational tractability.

\section{Regression}

We begin by introducing a simple regression problem, which we shall use as a running example throughout this chapter to motivate a number of key concepts. Suppose we observe a real-valued input variable $x$ and we wish to use this observation to predict the value of a real-valued target variable $y$. For the present purposes, it is instructive to consider an artificial example using synthetically generated data because we then know the precise process that generated the data for comparison against any learned model.

Now suppose that we are given a training set comprising $N$ observations of $x$, written $\mathbf{x} \equiv \left[ x_{1}, \dots, x_{N} \right]^{\mathrm{T}}$, together with corresponding observations of the values of $y$, denoted $\mathbf{y} \equiv\left[ y_{1}, \dots, y_{N} \right]^{\mathrm{T}}$. Figure ? shows a plot of a training set comprising $N = 10$ data points. The input data set $x$ in Figure ? was generated by choosing values of $x_n$ , for $n = 1, . . . , N$, spaced uniformly in range $[0, 1]$, and the target data set $y$ was obtained by first computing the corresponding values of the function $\sin(2\pi x)$ and then adding a small level of random noise having a Gaussian distribution to each such point in order to obtain the corresponding value $y_n$. By generating data in this way, we are capturing a property of many real data sets, namely that they possess an underlying regularity, which we wish to learn, but that individual observations are corrupted by random noise. This noise might arise from intrinsically stochastic (i.e. random) processes such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved.

Our goal is to exploit this training set in order to make predictions of the value $\mathbf{y}^*$ of the target variable for some new value $\mathbf{x}^*$ of the input variable. As we shall see later, this involves implicitly trying to discover the underlying function $\sin(2\pi x)$. This is intrinsically a difficult problem as we have to generalize from a finite data set. Furthermore the observed data are corrupted with noise, and so for a given $\mathbf{x}^*$ there is uncertainty as to the appropriate value for $\mathbf{y}^*$. Probability theory, provides a framework for expressing such uncertainty in a precise and quantitative manner \textcolor{red}{Section 1.2}.

For the moment, however, we shall proceed rather informally and consider a simple approach based on curve fitting. In particular, we shall fit the data using a polynomial function of the form
%
\begin{equation}
      f(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}
      \label{eq:intro-poly}
\end{equation}
%
where $M$ is the \textit{order} of the polynomial, and $x^j$ denotes $x$ raised to the power of $j$. The polynomial coefficients $w_0, \dots , w_M$ are collectively denoted by the vector $\mathbf{w}$. Note that, although the polynomial function $f(x, \mathbf{w})$ is a nonlinear function of $x$, it is a linear function of the coefficients $w$. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called \textit{linear models}. In general, we could write this weighted sum with any other function. In other words, we can put this in terms of $\phi_n(x)=x^n$, where $\phi$ could be other \textit{basis function}, e.g. we could have different functions $f$ for different basis functions.
%
\begin{align*}
      f(x,\mathbf{w}) &= w_0 \phi_0(x) +w_1 \phi_1(x) +w_2 \phi_2(x)  + ... + w_{M-1} \phi_{M-1}(x); \\
                        &= w_0 \exp\left\{ - \frac{(x-\mu_0)^2}{2\sigma^2}\right\} + w_1  \exp\left\{ - \frac{(x-\mu_1)^2}{2\sigma^2}\right\} + \\ & ... + w_{M-1} \exp\left\{ - \frac{(x-\mu_{M-1})^2}{2\sigma^2}\right\}; \\
                        &= w_0 \sin(0 \cdot x) + w_1 \cos(1 \cdot x) + \\ &... + w_{M-2} \sin((M-2) \cdot x) + w_{M-1} \cos((M-1) \cdot x); \\
                        &=\sum_{j=0}^{M-1} w_{j} \phi_{j}(x);
\end{align*}
%
For simplicity, we'll make use of an more compact notation with matrices (see Appendix \ref{app:matrix-form}). Then being $\boldsymbol{\phi}_m \equiv \left[  \phi_m(x_0) \dots \phi_m(x_N) \right]^{\mathrm{T}}$, we define the \textit{design matrix} as $\Phi \equiv \left[ \boldsymbol{\phi}_0 \cdots \boldsymbol{\phi}_{M-1} \right]$. Some textbooks use the notation $\mathit{X}$ for the design matrix.

The values of the coefficients will be determined by fitting the polynomial to the training data. This can be done by minimizing an \textit{error function} that measures the misfit between the function $f(x, \mathbf{w})$, for any given value of $w$, and the training set data points. One simple choice of error function, which is widely used, is given by the sum of the squares of the errors between the predictions $f(x_n, \mathbf{w})$ for each data point $x_n$ and the corresponding target values $y_n$, so that we minimize
%
\begin{equation}
      E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{f\left(x_{n}, \mathbf{w}\right)-y_{n}\right\}^{2}
\end{equation}
%
where the factor of $1/2$ is included for later convenience\footnote{Minkowski loss function}. We can solve the curve fitting problem by choosing the value of $w$ for which $E(\mathbf{w})$ is as small as possible. Because the error function is a quadratic function of the coefficients $\mathbf{w}$, its derivatives with respect to the coefficients will be linear in the elements of $\mathbf{w}$, and so the minimization of the error function has a unique solution, denoted by $\mathbf{w}^*$, which can be found in closed form (see Appendix \ref{app:parameters-optimization}). The resulting polynomial is given by the function $f(x,\mathbf{w}^*)$.


Papers to be submitted to NIPS 2013 must be prepared according to the
instructions presented here. Papers may be only up to eight pages long,
including figures. Since 2009 an additional ninth page \textit{containing only
   cited references} is allowed. Papers that exceed nine pages will not be
reviewed, or in any other way considered for presentation at the conference.
%This is a strict upper bound. 

Please note that this year we have introduced automatic line number generation
into the style file (for \LaTeXe and Word versions). This is to help reviewers
refer to specific lines of the paper when they make their comments. Please do
NOT refer to these line numbers in your paper as they will be removed from the
style file for the final version of accepted papers.

The margins in 2013 are the same as since 2007, which allow for $\approx 15\%$
more words in the paper compared to earlier years. We are also again using
double-blind reviewing. Both of these require the use of new style files.

Authors are required to use the NIPS \LaTeX{} style files obtainable at the
NIPS website as indicated below. Please make sure you use the current files and
not previous versions. Tweaking the style files may be grounds for rejection.

%% \subsection{Double-blind reviewing}

%% This year we are doing double-blind reviewing: the reviewers will not know 
%% who the authors of the paper are. For submission, the NIPS style file will 
%% automatically anonymize the author list at the beginning of the paper.

%% Please write your paper in such a way to preserve anonymity. Refer to
%% previous work by the author(s) in the third person, rather than first
%% person. Do not provide Web links to supporting material at an identifiable
%% web site.

%%\subsection{Electronic submission}
%%
%% \textbf{THE SUBMISSION DEADLINE IS MAY 31st, 2013. SUBMISSIONS MUST BE LOGGED BY
%% 23:00, MAY 31st, 2013, UNIVERSAL TIME}

%% You must enter your submission in the electronic submission form available at
%% the NIPS website listed above. You will be asked to enter paper title, name of
%% all authors, keyword(s), and data about the contact
%% author (name, full address, telephone, fax, and email). You will need to
%% upload an electronic (postscript or pdf) version of your paper.

%% You can upload more than one version of your paper, until the
%% submission deadline. We strongly recommended uploading your paper in
%% advance of the deadline, so you can avoid last-minute server congestion.
%%
%% Note that your submission is only valid if you get an e-mail
%% confirmation from the server. If you do not get such an e-mail, please
%% try uploading again. 


\subsection{Retrieval of style files}

The style files for NIPS and other conference information are available on the World Wide Web at
\begin{center}
   \url{http://www.nips.cc/}
\end{center}
The file \verb+nips2013.pdf+ contains these
instructions and illustrates the
various formatting requirements your NIPS paper must satisfy. \LaTeX{}
users can choose between two style files:
\verb+nips11submit_09.sty+ (to be used with \LaTeX{} version 2.09) and
\verb+nips11submit_e.sty+ (to be used with \LaTeX{}2e). The file
\verb+nips2013.tex+ may be used as a ``shell'' for writing your paper. All you
have to do is replace the author, title, abstract, and text of the paper with
your own. The file
\verb+nips2013.rtf+ is provided as a shell for MS Word users.

The formatting instructions contained in these style files are summarized in
sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

%% \subsection{Keywords for paper submission}
%% Your NIPS paper can be submitted with any of the following keywords (more than one keyword is possible for each paper):

%% \begin{verbatim}
%% Bioinformatics
%% Biological Vision
%% Brain Imaging and Brain Computer Interfacing
%% Clustering
%% Cognitive Science
%% Control and Reinforcement Learning
%% Dimensionality Reduction and Manifolds
%% Feature Selection
%% Gaussian Processes
%% Graphical Models
%% Hardware Technologies
%% Kernels
%% Learning Theory
%% Machine Vision
%% Margins and Boosting
%% Neural Networks
%% Neuroscience
%% Other Algorithms and Architectures
%% Other Applications
%% Semi-supervised Learning
%% Speech and Signal Processing
%% Text and Language Applications

%% \end{verbatim}

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
preferred typeface throughout. Paragraphs are separated by 1/2~line space,
with no indentation.

Paper title is 17~point, initial caps/lower case, bold, centered between
2~horizontal rules. Top rule is 4~points thick and bottom rule is 1~point
thick. Allow 1/4~inch space above and below title to rules. All pages should
start at 1~inch (6~picas) from the top of the page.

%The version of the paper submitted for review should have ``Anonymous Author(s)'' as the author of the paper.

For the final version, authors' names are
set in boldface, and each name is centered above the corresponding
address. The lead author's name is to be listed first (left-most), and
the co-authors' names (if different address) are set to follow. If
there is only one co-author, list both author and co-author side by side.

Please pay special attention to the instructions in section \ref{others}
regarding figures, tables, acknowledgments, and references.

\section{Headings: first level}
\label{headings}

First level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 12. One line space before the first level
heading and 1/2~line space after the first level heading.

\subsection{Headings: second level}

Second level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be numbered consecutively. The corresponding
number is to appear enclosed in square brackets, such as [1] or [2]-[5]. The
corresponding references are to be listed in the same order at the end of the
paper, in the \textbf{References} section. (Note: the standard
\textsc{Bib\TeX} style \texttt{unsrt} produces this.) As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

As submission is double blind, refer to your own published work in the
third person. That is, use ``In the previous work of Jones et al.\ [4]'',
not ``In our previous work [4]''. If you cite your other papers that
are not widely available (e.g.\ a journal paper under review), use
anonymous author names in the citation, e.g.\ an author of the
form ``A.\ Anonymous''.


\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
   \begin{center}
      %\framebox[4.0in]{$\;$}
      \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
   \end{center}
   \caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
   \caption{Sample table title}
   \label{sample-table}
   \begin{center}
      \begin{tabular}{ll}
         \multicolumn{1}{c}{\bf PART} & \multicolumn{1}{c}{\bf DESCRIPTION}
         \\ \hline \\
         Dendrite                     & Input terminal                      \\
         Axon                         & Output terminal                     \\
         Soma                         & Cell body (contains cell nucleus)   \\
      \end{tabular}
   \end{center}
\end{table}

\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PostScript or PDF files}

Please prepare PostScript or PDF files with paper size ``US Letter'', and
not, for example, ``A4''. The -t
letter option on dvips will produce US Letter files.

Fonts were the main cause of problems in the past years. Your PDF file must
only contain Type 1 or Embedded TrueType fonts. Here are a few instructions
to achieve this.

\begin{itemize}

   \item You can check which fonts a PDF files uses.  In Acrobat Reader,
         select the menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
         also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
         available out-of-the-box on most Linux machines.

   \item The IEEE has recommendations for generating PDF files whose fonts
         are also acceptable for NIPS. Please see
         \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

   \item LaTeX users:

         \begin{itemize}

            \item Consider directly generating PDF files using \verb+pdflatex+
                  (especially if you are a MiKTeX user).
                  PDF figures must be substituted for EPS figures, however.

            \item Otherwise, please generate your PostScript and PDF files with the following commands:
                  \begin{verbatim} 
dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
ps2pdf mypaper.ps mypaper.pdf
\end{verbatim}

                  Check that the PDF files only contains Type 1 fonts.
                  %For the final version, please send us both the Postscript file and
                  %the PDF file. 

            \item xfig "patterned" shapes are implemented with
                  bitmap fonts.  Use "solid" shapes instead.
            \item The \verb+\bbold+ package almost always uses bitmap
                  fonts.  You can try the equivalent AMS Fonts with command
                  \begin{verbatim}
\usepackage[psamsfonts]{amssymb}
\end{verbatim}
                  or use the following workaround for reals, natural and complex:
                  \begin{verbatim}
\newcommand{\RR}{I\!\!R} %real numbers
\newcommand{\Nat}{I\!\!N} %natural numbers 
\newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}

            \item Sometimes the problematic fonts are used in figures
                  included in LaTeX files. The ghostscript program \verb+eps2eps+ is the simplest
                  way to clean such figures. For black and white figures, slightly better
                  results can be achieved with program \verb+potrace+.
         \end{itemize}
   \item MSWord and Windows users (via PDF file):
         \begin{itemize}
            \item Install the Microsoft Save as PDF Office 2007 Add-in from
                  \url{http://www.microsoft.com/downloads/details.aspx?displaylang=en\&familyid=4d951911-3e7e-4ae6-b059-a2e79ed87041}
            \item Select ``Save or Publish to PDF'' from the Office or File menu
         \end{itemize}
   \item MSWord and Mac OS X users (via PDF file):
         \begin{itemize}
            \item From the print menu, click the PDF drop-down box, and select ``Save
                  as PDF...''
         \end{itemize}
   \item MSWord and Windows users (via PS file):
         \begin{itemize}
            \item To create a new printer
                  on your computer, install the AdobePS printer driver and the Adobe Distiller PPD file from
                  \url{http://www.adobe.com/support/downloads/detail.jsp?ftpID=204} {\it Note:} You must reboot your PC after installing the
                  AdobePS driver for it to take effect.
            \item To produce the ps file, select ``Print'' from the MS app, choose
                  the installed AdobePS printer, click on ``Properties'', click on ``Advanced.''
            \item Set ``TrueType Font'' to be ``Download as Softfont''
            \item Open the ``PostScript Options'' folder
            \item Select ``PostScript Output Option'' to be ``Optimize for Portability''
            \item Select ``TrueType Font Download Option'' to be ``Outline''
            \item Select ``Send PostScript Error Handler'' to be ``No''
            \item Click ``OK'' three times, print your file.
            \item Now, use Adobe Acrobat Distiller or ps2pdf to create a PDF file from
                  the PS file. In Acrobat, check the option ``Embed all fonts'' if
                  applicable.
         \end{itemize}

\end{itemize}
If your file contains Type 3 fonts or non embedded TrueType fonts, we will
ask you to fix it.

\subsection{Margins in LaTeX}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+
from the graphicx package. Always specify the figure width as a multiple of
the line width as in the example below using .eps graphics
\begin{verbatim}
   \usepackage[dvips]{graphicx} ... 
   \includegraphics[width=0.8\linewidth]{myfile.eps} 
\end{verbatim}
or % Apr 2009 addition
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ... 
   \includegraphics[width=0.8\linewidth]{myfile.pdf} 
\end{verbatim}
for .pdf graphics.
See section 4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

A number of width problems arise when LaTeX cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command.


\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the
final paper.

\include{appendix}


\printbibliography



\end{document}