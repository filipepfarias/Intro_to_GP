% !TeX root = ./Intro_to_GP.tex
\documentclass[11pt]{article} % For LaTeX2e
\usepackage{Intro_to_GP}

\title{Introduction to Gaussian Processes}

\IntroGPfinaltrue

\author{
Filipe P.~Farias \\
Teleinformatics Engineering Department\\
Federal University of Ceará\\
\texttt{filipepfarias@fisica.ufc.br} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
   A wide variety of methods exists to deal with supervised learning, as restrict a class of linear functions of the inputs, as linear regression, or give a prior probability to every possible function, giving high probability to the functions we consider more likely. The second approach is a way to Gaussian process itself. We will make the pathway through a intuitive construction of this framework.
\end{abstract}

\section{Introduction}

\lipsum[1]%

\section{Linear Regression}

Starting with a simple regression problem. Be the dataset $\mathcal{D}=\left\{ x_i,y_i|i=0,\dots,N-1 \right\}$, where we observe a real-valued input variable $x$ and a measured real-valued variable $y$. Then, we'll use synthetically generated data for comparison against any learned \textit{model}. And $N$ will be the number of observations of the value $y$. Our objective is make predictions of the new value $\hat{y}$ for some new input $\hat{x}$.

For this example, we'll use a simple approach based on curve fitting by the polynomial model, i.e., being the function

\begin{equation}
   f(x,\mathbf{w}) = \sum_{j=0}^{M-1} w_j x^j
\end{equation}

where $M$ is the order of the polynomial and $\mathbf{w}=\left[ w_0,\dots,w_M \right]$ its coefficients. It's important to note that the $f$ isn't linear in $x$ but in $\mathbf{w}$. These functions which are linear on the unknown parameters are called \textit{linear models}.
\textcolor{red}{Section 1.1 - Bishop (pg 4).}

We can extend the class of models considering linear combinations of nonlinear functions of the input variables, i.e.

\begin{equation}
   f(x,\mathbf{w}) = \sum_{j=0}^{M-1} \phi_j (x) w_j 
   \label{eq:lin-reg-model-1}
\end{equation}

where $\phi_j (x)$ are known as \textit{basis functions}, and then the total number of parameters for this model will be $M$. We can evaluate the same operation of (\ref{eq:lin-reg-model-1}) in the matrix form by

\begin{equation}
   f(x,\mathbf{w}) = \boldsymbol{\phi}(x)^\top  \mathbf{w}
   \label{eq:lin-reg-model-1-matrix-form}
\end{equation}

where $\boldsymbol{\phi}(x) = \left[ \phi_0(x), \dots, \phi_{M-1}(x) \right]^\top$. In the example of the curve fitting, the polynomial regression implies that $\phi_j(x)=x^j$. It's important to note that these linear models are needed to define its basis functions before the training dataset is observed. \textcolor{red}{Section 1.4 - Bishop (pg 33).}

The values of $\mathbf{w}$ are obtained by minimizing the \textit{error function}, a measure of the distance between the training dataset and $f$, given values of $\mathbf{w}$. By the way, the chosen error function will be 

\begin{equation}
   E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\{ f(x_i,\mathbf{w})-y_i \right\}^2
   \label{eq:lin-reg-error-function-1}
\end{equation}

This indicate that if $E$ is zero, $f$ passes exactly through each training data point. Observe that $E$ do not assume negative values because of its quadratic form, then we can find $\mathbf{w}$ by finding the minimum value of $E$, denoted $\mathbf{w^*}$, by

\begin{equation}
   \frac{\partial E}{\partial \mathbf{w}} = 0
\end{equation}

We can rewrite (\ref{eq:lin-reg-error-function-1}) in the matrix form, considering $\mathbf{f} = f(\mathbf{x},\mathbf{w})$, where $\mathbf{x}=\left[ x_0,\dots, x_{N-1} \right]^\top$, i.e. $f$ evaluated for all input variables (See Appendix \ref{subsec:app-matrix-form}). Then we have

\begin{equation}
   \label{eq:lin-reg-error-function-matrix-form}
   \mathbf{f} = \Phi \mathbf{w}
\end{equation}

where $\Phi$ is the \textit{design matrix} such that $\boldsymbol{\phi}(x)$ is evaluated for all $\mathbf{x}$. Proceeding with the minimization, we obtain the optimal $\mathbf{w}$, or $\mathbf{w}^*$ by

\begin{equation}
   \label{eq:lin-reg-opt-param}
   \mathbf{w}^{*} = (\Phi^\top \Phi)^{-1}\Phi^\top \mathbf{y}
\end{equation}

which are the parameters that best fit the model to the data.

\textcolor{red}{Insert over-fitting.}

As we increase the number of parameters, our model becomes more flexible and then our error function approximates of zero for the training data. But when compared to the test data, the error increases. This is known as \textit{over-fitting}.

\subsection{Regularized Linear Regression}

An approach to minimize the over-fitting problem is to control the flexibility of the model. This can be done by controlling the norm of $\mathbf{w}^*$ as the number of parameters increases. By (\ref{eq:lin-reg-error-function-1}) we can add the penalty term $||\mathbf{w}||^2 $ scaled by the factor $\lambda / 2$, then

\begin{equation}
   \label{eq:lin-reg-error-function-regularized}
   E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\{ f(x_i,\mathbf{w})-y_i \right\}^2 + \frac{\lambda}{2} ||\mathbf{w}||^2
\end{equation}

whats means that our error increases as the norm of $\mathbf{w}$ grows. This will lead us to the matrix form

\begin{equation}
   \label{eq:lin-reg-error-function-regularized-matrix-form}
   \mathbf{w}^{*} = (\Phi^\top \Phi + \lambda \mathbf{I})^{-1}\Phi^\top \mathbf{y} 
\end{equation}

There are many example of choices for basis functions, as

\begin{equation}
   \phi_j(x) = \exp \left\{ -\frac{\left(x-\mu_j\right)^2}{2s^2} \right\}
\end{equation}

known as \textit{squared exponential}, where $\mu_j$ controls the location of the basis function in the \textit{input space}, and $s$ the spatial scale. It's usually referred as 'Gaussian' basis function because of its similarity with the Gaussian distribution function, although there is no probabilistic interpretation here.

% \begin{figure}[H]
%    \centering
%    \includegraphics[width=.25\textwidth]{Figures/f1.png}
%    \caption{Símbolos do diagrama de blocos. (a) Adição de duas seqüências. (b) Multiplicação de uma sequência por um  constante. c) atraso da unidade.}
%    \label{graph:A-and-B-systems}
% \end{figure}

\lipsum[1]

\newpage
\input{Intro_to_GP_appendices}

\newpage
\bibliography{Intro_to_GP_bibliography}

\end{document}