\section*{\LARGE{Appendix}}

\begin{appendices}
  \section{Derivations}
  
  \subsection{Matrix Form}
  \label{subsec:app-matrix-form}
  
  Be the linear model $f(x,\mathbf{w}) = \mathbf{w}^\top \boldsymbol{\phi}(x)$. Suppose $\Phi = \left[ \boldsymbol{\phi}(x_1),\dots,\boldsymbol{\phi}(x_N) \right]^\top$, then $\Phi$ will be of the form


\begin{equation}
    \Phi =
    \begin{bmatrix}
      \phi_0(x_0) & \dots & \phi_{M-1}(x_{0}) \\
      \vdots & \ddots & \vdots \\
      \phi_{0}(x_{N-1}) & \dots & \phi_{M-1}(x_{N-1})
    \end{bmatrix}
\end{equation}

called \textit{design matrix}. Then the model turns to $\mathbf{f} = \Phi \mathbf{w}$. This will lead us to the matrix form for the quadratic error function

\begin{align*}
  E(\mathbf{w}) &= \frac{1}{2}(\mathbf{f} - \mathbf{y})^\top (\mathbf{f} - \mathbf{y}) \\
                &= \frac{1}{2}(\Phi \mathbf{w} - \mathbf{y})^\top (\Phi \mathbf{w} - \mathbf{y}) \\
                &= \frac{1}{2}(\mathbf{w}^\top \Phi^\top \Phi \mathbf{w}  - \mathbf{y}^\top \Phi \mathbf{w} - \mathbf{w}^\top \Phi^\top \mathbf{y} + \mathbf{y}^\top \mathbf{y}) \nonumber \\
\end{align*}

Observe that even in the matrix form, the error function remains scalar, which implies that $\mathbf{y}^\top \Phi \mathbf{w} = \mathbf{w}^\top \Phi^\top \mathbf{y}$ by the transpose of the product rule. Then

\begin{align*}
  E(\mathbf{w}) &= \frac{1}{2}(\mathbf{w}^\top \Phi^\top \Phi \mathbf{w}  - 2\mathbf{y}^\top \Phi \mathbf{w} + \mathbf{y}^\top \mathbf{y}) \nonumber \\
\end{align*}

Then we proceed by the minimization by $\frac{\partial E}{\partial \mathbf{w}} = 0$

\begin{align}
  0 &= \frac{1}{2}(2\mathbf{w}^\top \Phi^\top \Phi - 2\mathbf{y}^\top \Phi) \footnotemark[2] \nonumber \\
  \mathbf{w}^{*\top} &= \mathbf{y}^\top \Phi (\Phi^\top \Phi) ^{-1} \nonumber \\
  \mathbf{w}^{*} &= (\Phi^\top \Phi)^{-1}\Phi^\top \mathbf{y} 
\end{align}\footnotetext[2]{Using two facts. First, if $\alpha=\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}$, then $\frac{\partial \alpha}{\partial \mathbf{x}}=2 \mathbf{x}^{\top} \mathbf{A}$, being $\alpha$ scalar. Second, if $\alpha=\mathbf{y}^{\mathrm{T}} \mathbf{A} \mathbf{x}$, then $\frac{\partial \alpha}{\partial \mathbf{x}}=\mathbf{y}^{\mathrm{T}} \mathbf{A}$. For both, $\mathbf{A}$ is independent of $\mathbf{x}$ and $\mathbf{y}$ \cite{graybill1983matrices}.}

For the regularized linear regression, we do $\frac{\lambda}{2} ||\mathbf{w}||^2 = \mathbf{w}^\top \mathbf{w}$, then

\begin{align*}
  E(\mathbf{w}) &= \frac{1}{2}(\mathbf{f} - \mathbf{y})^\top (\mathbf{f} - \mathbf{y}) + \frac{\lambda}{2} \mathbf{w}^\top \mathbf{w}\\
                &= \frac{1}{2}(\mathbf{w}^\top \Phi^\top \Phi \mathbf{w}  - \mathbf{y}^\top \Phi \mathbf{w} - \mathbf{w}^\top \Phi^\top \mathbf{y} + \mathbf{y}^\top \mathbf{y}) + \frac{\lambda}{2} \mathbf{w}^\top \mathbf{w} \nonumber \\
\end{align*}

And with the minimization we do $\frac{\partial E}{\partial \mathbf{w}} = 0$, then

\begin{align}
  0 &= \mathbf{w}^\top \Phi^\top \Phi - \mathbf{y}^\top \Phi + \lambda \mathbf{w}^\top\nonumber \\
  \mathbf{w}^{*\top} &= \mathbf{y}^\top \Phi (\Phi^\top \Phi + \lambda \mathbf{I}) ^{-1} \nonumber \\
  \mathbf{w}^{*} &= (\Phi^\top \Phi + \lambda \mathbf{I})^{-1}\Phi^\top \mathbf{y} 
\end{align}

where $\mathbf{I}$ is the identity matrix.

  \section{Partitioned Gaussian distributions\cite{schon_lindsten}}.

  Be $\mathbf{x}$ a n-dimensional vector with a Gaussian distribution $\mathcal{N}\left( \mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma} \right)$, then the partitioned will be
  
  \begin{equation}
    \mathbf{x}=
    \begin{pmatrix}
    \mathbf{x}_a \\  
    \mathbf{x}_b 
    \end{pmatrix}
    ,\quad 
    \boldsymbol{\mu}=
    \begin{pmatrix}
      \boldsymbol{\mu}_a \\
      \boldsymbol{\mu}_b
    \end{pmatrix}
    ,\quad 
    \boldsymbol{\Sigma}=
    \begin{pmatrix}
      \boldsymbol{\Sigma}_{aa} & \boldsymbol{\Sigma}_{ab}  \\
      \boldsymbol{\Sigma}_{ba} & \boldsymbol{\Sigma}_{bb}
    \end{pmatrix} = 
    .
  \end{equation}

  Preserved the symmetry $\boldsymbol{\Sigma}^\top = \boldsymbol{\Sigma}$, we say the covariance matrix is positive definite. And be the multivariate Gaussian

  \begin{equation}
    \label{eq:app-par-gau-multivariate-gaussian}
    \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})=\frac{1}{(2 \pi)^{n / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}
  \end{equation}

  We define too, just for convenience of work, the precision matrix $\boldsymbol{\Lambda}$ by

  \begin{equation}
    \boldsymbol{\Lambda} = 
    \begin{pmatrix}
      \boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab}  \\
      \boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb}
    \end{pmatrix} 
    \equiv \boldsymbol{\Sigma}^{-1}
  \end{equation}

  \begin{theorem}[Marginalization]
    Being the random vector $\mathbf{x}$ and its partitioned as above, the marginal density $p(\mathbf{x}_a)$ is given by
    \begin{displaymath}
      p(\mathbf{x}_a) = \mathcal{N}\left( \mathbf{x}_a | \boldsymbol{\mu}_a, \boldsymbol{\Sigma}_{aa} \right)
    \end{displaymath}
  \end{theorem}

  \begin{proof}
    The marginal density $p(\mathbf{x}_a)$ is obtained by integrating the joint density $p(\mathbf{x})=p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right)$ with relation to $\mathbf{x}_b$
    \begin{equation}
      p\left(\mathbf{x}_{a}\right)=\int p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right) \mathrm{d} \mathbf{x}_{b}
    \end{equation}

    Then we expand the exponential argument of (\ref{eq:app-par-gau-multivariate-gaussian}) for the partitioned Gaussian

    \begin{equation}
      -\frac{1}{2}
      \left( \begin{pmatrix}
        \mathbf{x}_a \\  
        \mathbf{x}_b 
        \end{pmatrix}
        -\begin{pmatrix}
          \boldsymbol{\mu}_a \\
          \boldsymbol{\mu}_b
        \end{pmatrix}
        \right)^\top
        \begin{pmatrix}
          \boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab}  \\
          \boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb}
        \end{pmatrix} 
        \left( \begin{pmatrix}
          \mathbf{x}_a \\  
          \mathbf{x}_b 
          \end{pmatrix}
          -\begin{pmatrix}
            \boldsymbol{\mu}_a \\
            \boldsymbol{\mu}_b
          \end{pmatrix}
          \right)
    \end{equation}
    
    What implies

    \begin{equation}
    \begin{aligned} &-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \\ &-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) 
    \end{aligned}
    \end{equation}

    Here we make use of the \textit{Schur complement}, in which, being the partitioned matrix

    \begin{equation}
    \left(\begin{array}{cc}{\mathbf{A}} & {\mathbf{B}} \\ {\mathbf{C}} & {\mathbf{D}}\end{array}\right)^{-1}=\left(\begin{array}{cc}{\mathbf{M}} & {-\mathbf{M B D}^{-1}} \\ {-\mathbf{D}^{-1} \mathbf{C M}} & {\mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{C M B D}^{-1}}\end{array}\right)
    \end{equation}

    the quantity $\mathbf{M}^{-1}$ is the Schur complement, defined as 

    \begin{equation}
      \mathbf{M}=\left(\mathbf{A}-\mathbf{B D}^{-1} \mathbf{C}\right)^{-1}  
    \end{equation}

    This will motivated the term grouping below

    \begin{equation}
    \begin{aligned} &-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \\ &-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \\ &=-\frac{1}{2}\left(\mathbf{x}_{b}^\top \boldsymbol{\Lambda}_{b b} \mathbf{x}_{b}-2 \mathbf{x}_{b}^\top \boldsymbol{\Lambda}_{b b}\left(\boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right)-2 \mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a b} \boldsymbol{\mu}_{b}\right.\\ & \left.  +2 \boldsymbol{\mu}_{a}^\top \boldsymbol{\Lambda}_{a b} \boldsymbol{\mu}_{b}+\boldsymbol{\mu}_{b}^\top \boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_{b}+\mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a a} \mathbf{x}_{a}-2 \mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_{a}+\boldsymbol{\mu}_{a}^\top \boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_{a} \right) \end{aligned}
    \end{equation}
  \end{proof}
  
  

  \subsection{Sub}

\end{appendices}