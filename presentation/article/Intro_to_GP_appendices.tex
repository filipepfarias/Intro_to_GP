\section*{\LARGE{Appendix}}

\begin{appendices}
  \vspace{4em}
  \section{Derivations}
  
  \subsection{Matrix Form}
  \label{subsec:app-matrix-form}
  
  Be the linear model $f(x,\mathbf{w}) = \mathbf{w}^\top \boldsymbol{\phi}(x)$. Suppose $\Phi = \left[ \boldsymbol{\phi}(x_1),\dots,\boldsymbol{\phi}(x_N) \right]^\top$, then $\Phi$ will be of the form


\begin{equation}
    \Phi =
    \begin{bmatrix}
      \phi_0(x_0) & \dots & \phi_{M-1}(x_{0}) \\
      \vdots & \ddots & \vdots \\
      \phi_{0}(x_{N-1}) & \dots & \phi_{M-1}(x_{N-1})
    \end{bmatrix}
\end{equation}

called \textit{design matrix}. Then the model turns to $\mathbf{f} = \Phi \mathbf{w}$. This will lead us to the matrix form for the quadratic error function

\begin{align*}
  E(\mathbf{w}) &= \frac{1}{2}(\mathbf{f} - \mathbf{y})^\top (\mathbf{f} - \mathbf{y}) \\
                &= \frac{1}{2}(\Phi \mathbf{w} - \mathbf{y})^\top (\Phi \mathbf{w} - \mathbf{y}) \\
                &= \frac{1}{2}(\mathbf{w}^\top \Phi^\top \Phi \mathbf{w}  - \mathbf{y}^\top \Phi \mathbf{w} - \mathbf{w}^\top \Phi^\top \mathbf{y} + \mathbf{y}^\top \mathbf{y}) \nonumber \\
\end{align*}

Observe that even in the matrix form, the error function remains scalar, which implies that $\mathbf{y}^\top \Phi \mathbf{w} = \mathbf{w}^\top \Phi^\top \mathbf{y}$ by the transpose of the product rule. Then

\begin{align*}
  E(\mathbf{w}) &= \frac{1}{2}(\mathbf{w}^\top \Phi^\top \Phi \mathbf{w}  - 2\mathbf{y}^\top \Phi \mathbf{w} + \mathbf{y}^\top \mathbf{y}) \nonumber \\
\end{align*}

Then we proceed by the minimization by $\frac{\partial E}{\partial \mathbf{w}} = 0$

\begin{align}
  0 &= \frac{1}{2}(2\mathbf{w}^\top \Phi^\top \Phi - 2\mathbf{y}^\top \Phi) \footnotemark[2] \nonumber \\
  \mathbf{w}^{*\top} &= \mathbf{y}^\top \Phi (\Phi^\top \Phi) ^{-1} \nonumber \\
  \mathbf{w}^{*} &= (\Phi^\top \Phi)^{-1}\Phi^\top \mathbf{y} 
\end{align}\footnotetext[2]{Using two facts. First, if $\alpha=\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}$, then $\frac{\partial \alpha}{\partial \mathbf{x}}=2 \mathbf{x}^{\top} \mathbf{A}$, being $\alpha$ scalar. Second, if $\alpha=\mathbf{y}^{\mathrm{T}} \mathbf{A} \mathbf{x}$, then $\frac{\partial \alpha}{\partial \mathbf{x}}=\mathbf{y}^{\mathrm{T}} \mathbf{A}$. For both, $\mathbf{A}$ is independent of $\mathbf{x}$ and $\mathbf{y}$ \cite{graybill1983matrices}.}

For the regularized linear regression, we do $\frac{\lambda}{2} ||\mathbf{w}||^2 = \mathbf{w}^\top \mathbf{w}$, then

\begin{align*}
  E(\mathbf{w}) &= \frac{1}{2}(\mathbf{f} - \mathbf{y})^\top (\mathbf{f} - \mathbf{y}) + \frac{\lambda}{2} \mathbf{w}^\top \mathbf{w}\\
                &= \frac{1}{2}(\mathbf{w}^\top \Phi^\top \Phi \mathbf{w}  - \mathbf{y}^\top \Phi \mathbf{w} - \mathbf{w}^\top \Phi^\top \mathbf{y} + \mathbf{y}^\top \mathbf{y}) + \frac{\lambda}{2} \mathbf{w}^\top \mathbf{w} \nonumber \\
\end{align*}

And with the minimization we do $\frac{\partial E}{\partial \mathbf{w}} = 0$, then

\begin{align}
  0 &= \mathbf{w}^\top \Phi^\top \Phi - \mathbf{y}^\top \Phi + \lambda \mathbf{w}^\top\nonumber \\
  \mathbf{w}^{*\top} &= \mathbf{y}^\top \Phi (\Phi^\top \Phi + \lambda \mathbf{I}) ^{-1} \nonumber \\
  \mathbf{w}^{*} &= (\Phi^\top \Phi + \lambda \mathbf{I})^{-1}\Phi^\top \mathbf{y} 
\end{align}

where $\mathbf{I}$ is the identity matrix.

\vspace{4em}

  \section{Bayes' theorem for Gaussian variables\cite{schon_lindsten}}
  \label{subsec:app-par-gau}

  \subsection{Partitioned Gaussian distributions}
  Be $\mathbf{x}$ a n-dimensional vector with a Gaussian distribution $\mathcal{N}\left( \mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma} \right)$, then the partitioned will be
  
  \begin{equation}
    \mathbf{x}=
    \begin{pmatrix}
    \mathbf{x}_a \\  
    \mathbf{x}_b 
    \end{pmatrix}
    ,\quad 
    \boldsymbol{\mu}=
    \begin{pmatrix}
      \boldsymbol{\mu}_a \\
      \boldsymbol{\mu}_b
    \end{pmatrix}
    ,\quad 
    \boldsymbol{\Sigma}=
    \begin{pmatrix}
      \boldsymbol{\Sigma}_{aa} & \boldsymbol{\Sigma}_{ab}  \\
      \boldsymbol{\Sigma}_{ba} & \boldsymbol{\Sigma}_{bb}
    \end{pmatrix}
    .
  \end{equation}

  Preserved the symmetry $\boldsymbol{\Sigma}^\top = \boldsymbol{\Sigma}$, we say the covariance matrix is positive definite. And be the multivariate Gaussian

  \begin{equation}
    \label{eq:app-par-gau-multivariate-gaussian}
    \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})=\frac{1}{(2 \pi)^{n / 2}} \frac{1}{ \left( \det \mathbf{\Sigma} \right) ^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}
  \end{equation}

  We define too, just for convenience of work, the precision matrix $\boldsymbol{\Lambda}$ by

  \begin{equation}
    \boldsymbol{\Lambda} = 
    \begin{pmatrix}
      \boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab}  \\
      \boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb}
    \end{pmatrix} 
    \equiv \boldsymbol{\Sigma}^{-1}
  \end{equation}

  assuming all matrices have inverses.

  \begin{theorem}[Marginalization]
    \label{theorem:app-gau-margin}
    Being the random vector $\mathbf{x}$ and its partitioned as above, the marginal density $p(\mathbf{x}_a)$ is given by
    \begin{displaymath}
      p(\mathbf{x}_a) = \mathcal{N}\left( \mathbf{x}_a | \boldsymbol{\mu}_a, \boldsymbol{\Sigma}_{aa} \right)
    \end{displaymath}
  \end{theorem}

  \begin{proof}
    The marginal density $p(\mathbf{x}_a)$ is obtained by integrating the joint density $p(\mathbf{x})=p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right)$ with relation to $\mathbf{x}_b$
    \begin{equation}
      p\left(\mathbf{x}_{a}\right)=\int p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right) \mathrm{d} \mathbf{x}_{b}
    \end{equation}

    Then we expand the exponential argument of (\ref{eq:app-par-gau-multivariate-gaussian}) for the partitioned Gaussian

    \begin{equation}
      -\frac{1}{2}
      \left( \begin{pmatrix}
        \mathbf{x}_a \\  
        \mathbf{x}_b 
        \end{pmatrix}
        -\begin{pmatrix}
          \boldsymbol{\mu}_a \\
          \boldsymbol{\mu}_b
        \end{pmatrix}
        \right)^\top
        \begin{pmatrix}
          \boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab}  \\
          \boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb}
        \end{pmatrix} 
        \left( \begin{pmatrix}
          \mathbf{x}_a \\  
          \mathbf{x}_b 
          \end{pmatrix}
          -\begin{pmatrix}
            \boldsymbol{\mu}_a \\
            \boldsymbol{\mu}_b
          \end{pmatrix}
          \right)
    \end{equation}
    
    What implies

    \begin{equation}
    \begin{aligned} &-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \\ &-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) 
    \end{aligned}
    \end{equation}

    Here we make use of the \textit{Schur complement}, in which, being the partitioned matrix

    \begin{equation}
    \left(\begin{array}{cc}{\mathbf{A}} & {\mathbf{B}} \\ {\mathbf{C}} & {\mathbf{D}}\end{array}\right)^{-1}=\left(\begin{array}{cc}{\mathbf{A}^{-1}+\mathbf{A}^{-1} \mathbf{B} \mathbf{M}^{-1} \mathbf{C} \mathbf{A}^{-1}} & {-\mathbf{A}^{-1} \mathbf{B} \mathbf{M}^{-1}} \\ {-\mathbf{M}^{-1} \mathbf{C} \mathbf{A}^{-1}} & {\mathbf{M}^{-1}}\end{array}\right)
    \end{equation}

    the quantity $\mathbf{M}^{-1}$ is the \textit{Schur complement} of the left side matrix with respect to $\mathbf{D}$, defined as 

    \begin{equation}
      \label{eq:app-par-gau-schur}
      \mathbf{M}=\left(\mathbf{D}-\mathbf{C A}^{-1} \mathbf{B}\right)^{-1}  
    \end{equation}

    This will motivated the term grouping below

    \begin{equation*}
    \begin{aligned} &-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \\ &-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^\top \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \\ &=-\frac{1}{2}\left(\mathbf{x}_{b}^\top \boldsymbol{\Lambda}_{b b} \mathbf{x}_{b}-2 \mathbf{x}_{b}^\top \boldsymbol{\Lambda}_{b b}\left(\boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right)-2 \mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a b} \boldsymbol{\mu}_{b}\right.\\ & \left.  +2 \boldsymbol{\mu}_{a}^\top \boldsymbol{\Lambda}_{a b} \boldsymbol{\mu}_{b}+\boldsymbol{\mu}_{b}^\top \boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_{b}+\mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a a} \mathbf{x}_{a}-2 \mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_{a}+\boldsymbol{\mu}_{a}^\top \boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_{a} \right) \end{aligned}
    \end{equation*}

    Then we complete the squares resulting independent

    \begin{equation*}
    \begin{aligned} &-\frac{1}{2}\left(\mathbf{x}_{b}-\left(\boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right)\right)^\top \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\left(\boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right)\right) \\ &+\frac{1}{2}\left(\mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a} \mathbf{x}_{a}-2\mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a} \boldsymbol{\mu}_{a}+\boldsymbol{\mu}_{a} \boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a} \boldsymbol{\mu}_{a}\right) \\ &-\frac{1}{2}\left(\mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a a} \mathbf{x}_{a}-2 \mathbf{x}_{a}^\top \boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_{a}+\boldsymbol{\mu}_{a}^\top \boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_{a}\right) \\ &=\underbrace{-\frac{1}{2}\left(\mathbf{x}_{b}-\left(\boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right)\right)^\top \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\left(\boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right)\right)}_{E_1} \\ &\underbrace{-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right)\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)}_{E_2} \end{aligned}
    \end{equation*}

    Now, back to the marginalization, we have

    \begin{equation}
      \label{eq:app-par-gau-marginalization-1}
      p\left(\mathbf{x}_{a}\right)=\int \frac{1}{(2 \pi)^{n / 2}} \frac{1}{ \left( \det \mathbf{\Sigma} \right) ^{1 / 2}} \exp \left\{ E_1 \right\} \exp \left\{ E_2 \right\} d \mathbf{x}_b
    \end{equation}

    By inspection, we have that, being the integral of the density function equals one and being $n_b$ the dimension of $\mathbf{x}_b$

    \begin{equation}
      \label{eq:app-par-gau-marginalization-2}
      \int \exp \left\{ E_1 \right\} d \mathbf{x}_b = \left( 2 \pi \right)^{n_b/2} \left( \det \mathbf{\Lambda}_{bb}^{-1} \right) ^{1 / 2}
    \end{equation}

    Then, substituting (\ref{eq:app-par-gau-marginalization-1}) in (\ref{eq:app-par-gau-marginalization-2}) we have

    \begin{equation*}
      p\left(\mathbf{x}_{a}\right)=\frac{1}{(2 \pi)^{n_a / 2}} \frac{\left( \det \mathbf{\Lambda}_{bb}^{-1} \right) ^{1 / 2}}{ \left( \det \mathbf{\Sigma} \right) ^{1 / 2}} \exp \left\{ E_2 \right\}
    \end{equation*}

    We will use the determinant property from Schur complement below

    \begin{equation}
    \label{eq:app-det-schur-complement}
    \det \left(\begin{array}{cc}{\mathbf{A}} & \mathbf{B} \\ \mathbf{C} & \mathbf{D} \end{array}\right)=\det \mathbf{A} \det \left(\mathbf{D}-\mathbf{C A}^{-1} \mathbf{B}\right)
    \end{equation}

    Then, substituting the values of $\boldsymbol{\Sigma}$ we have that

    \begin{equation}
      \det \boldsymbol{\Sigma}=\det \boldsymbol{\Sigma}_{a a} \det\left(\boldsymbol{\Sigma}_{b b}-\boldsymbol{\Sigma}_{b a} \boldsymbol{\Sigma}_{a a}^{-1} \boldsymbol{\Sigma}_{a b}\right)
    \end{equation}

    Using the Schur complement

    \begin{equation}
      \det \boldsymbol{\Sigma}=\det \boldsymbol{\Sigma}_{a a} \det \boldsymbol{\Lambda}_{bb}^{-1}
    \end{equation}

    Finally, using the Schur complement again, we obtain that $\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a} = \boldsymbol{\Sigma}_{a a}$, what concludes the proof, substituting the result in $E_2$.
  \end{proof}

  \begin{theorem}[Conditioning]
    \label{theorem:app-gau-condit}
    Being the random vector $\mathbf{x}$ and its partitioned as above, the conditional density $p(\mathbf{x}_b | \mathbf{x}_a)$ is given by
    \begin{displaymath}
      p(\mathbf{x}_b | \mathbf{x}_a) = \mathcal{N}\left( \mathbf{x}_a | \boldsymbol{\mu}_{b|a}, \boldsymbol{\Sigma}_{b|a} \right)
    \end{displaymath}
    where $\boldsymbol{\mu}_{b|a} = \boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)$ and $\boldsymbol{\Sigma}_{b|a} = \boldsymbol{\Lambda}_{aa}^{-1}$.
  \end{theorem}

  \begin{proof}
    By the product rule we have

    \begin{equation}
      p(\mathbf{x}_b|\mathbf{x}_a)=\frac{p(\mathbf{x})}{p(\mathbf{x}_a)}
    \end{equation}

    We already obtain the value for $p(\mathbf{x}_a)$, then

    \begin{equation}
      p(\mathbf{x}_b|\mathbf{x}_a)=\frac{(2 \pi )^{n_a/2}}{\left( 2 \pi \right)^{n/2}} \frac{( \det \mathbf{\Sigma}_{aa} )^{n_a/2}}{\left( \det \boldsymbol{\Sigma} \right)^{1/2}} \frac{\exp \left\{ E_1 \right\} \exp \left\{ E_2 \right\}}{\exp \left\{ E_2 \right\}}
    \end{equation}
    
    Similarly to what was done in \textit{marginalization} and using the Schur complement, the result

    \begin{equation}
      p(\mathbf{x}_b|\mathbf{x}_a)=\frac{1}{\left( 2 \pi \right)^{n_b/2}} \frac{1}{\left( \det \boldsymbol{\Lambda}_{bb} \right)^{1/2}} \exp \left\{ E_1 \right\}
    \end{equation}

    And finally, by inspection of $E_1$ we have

    \begin{subequations}
      \begin{align}
        \boldsymbol{\mu}_{b|a} &=  \boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right) \\
        \intertext{and analogously}
        \boldsymbol{\mu}_{a|b} &=  \boldsymbol{\mu}_{a}-\boldsymbol{\Lambda}_{a a}^{-1} \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)
      \end{align}
    \end{subequations}

    for $\boldsymbol{\Lambda}_{aa}$.
  \end{proof}

  \subsection{Affine transformation}
  \label{subsec:app-affine-transf-gau}
  
  Making use of the Theorems \ref{theorem:app-gau-margin} and \ref{theorem:app-gau-condit}, we can show that the Gaussian is closed under linear transformations, i.e. an affine transformation of Gaussians results in another Gaussian.
  
  \begin{theorem}[Affine transformation]
    
    Assume $\mathbf{x}_a$ and $\mathbf{x}_b$ are Gaussian distributed and $\mathbf{x}_b$ conditioned by $\mathbf{x}_a$, as

    \begin{equation}
    p\left(\mathbf{x}_{a}\right) =\mathcal{N}\left(\mathbf{x}_{a} | \boldsymbol{\mu}_{a}, \boldsymbol{\Sigma}_{a}\right), \quad p\left(\mathbf{x}_{b} | \mathbf{x}_{a}\right) =\mathcal{N}\left(\mathbf{x}_{b} | \mathbf{M} \mathbf{x}_{a}+\mathbf{d}, \boldsymbol{\Sigma}_{b | a}\right)
    \end{equation}

    where $\mathbf{M}$ is a constant matrix and $\mathbf{d}$ a constant vector, both with the appropriate dimensions. Then the joint distribution is given by

    \begin{equation}
    p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right)=\mathcal{N}\left(\left(\begin{array}{c}{\mathbf{x}_{a}} \\ {\mathbf{x}_{b}}\end{array}\right) \left|\left(\begin{array}{c}{\boldsymbol{\mu}_{a}} \\ {\mathbf{M} \boldsymbol{\mu}_{a}+\mathbf{d}}\end{array}\right.\right), \mathbf{R}\right),
    \end{equation}

    being

    \begin{equation}
    \mathbf{R}=\left(\begin{array}{cc}{\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}+\boldsymbol{\Sigma}_{a}^{-1}} & {-\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1}} \\ {-\boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}} & {\boldsymbol{\Sigma}_{b | a}^{-1}}\end{array}\right)^{-1}=\left(\begin{array}{cc}{\boldsymbol{\Sigma}_{a}} & {\boldsymbol{\Sigma}_{a} \mathbf{M}^\top} \\ {\mathbf{M} \boldsymbol{\Sigma}_{a}} & {\boldsymbol{\Sigma}_{b | a}+\mathbf{M} \boldsymbol{\Sigma}_{a} \mathbf{M}^\top}\end{array}\right).
    \end{equation}
    
  \end{theorem}

  \begin{proof}
    Being the vector

    \begin{equation}
      \mathbf{x} = \left( \begin{array}{c} \mathbf{x}_a \\ \mathbf{x}_b \end{array} \right)
    \end{equation}

    and its joint distribution, from the Theorems 1 and 2,

    \begin{equation}
      p(\mathbf{x}) = p(\mathbf{x}_b | \mathbf{x}_a)p(\mathbf{x}_a) = \frac{(2 \pi)^{-\left(n_{a}+n_{b}\right) / 2}}{\sqrt{\det \boldsymbol{\Sigma}_{b | a} \det \boldsymbol{\Sigma}_{a}}} \exp \left\{ -\frac{1}{2} E \right\}
    \end{equation}

    where 

    \begin{equation}
      E=\left(\mathbf{x}_{b}-\mathbf{M} \mathbf{x}_{a}-\mathbf{d}\right)^\top \boldsymbol{\Sigma}_{b | a}^{-1}\left(\mathbf{x}_{b}-\mathbf{M} \mathbf{x}_{a}-\mathbf{d}\right)+\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^\top \boldsymbol{\Sigma}_{a}^{-1}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right).
    \end{equation}

    Rewriting $\mathbf{x}_{b}-\mathbf{M} \mathbf{x}_{a}-\mathbf{d}$ as $f$ and $\mathbf{x}_{a}-\boldsymbol{\mu}_{a}$ as $e$, we have

    \begin{align}
      \label{eq:app-affine-transf-cov-matrix}
      E&=\left(f-\mathbf{M}e\right)^\top \boldsymbol{\Sigma}_{b | a}^{-1}\left(f-\mathbf{M}e\right)+\left(e\right)^\top \boldsymbol{\Sigma}_{a}^{-1}\left(e\right) \nonumber \\
      &= e^\top \left(\mathbf{M}^\top  \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}+\boldsymbol{\Sigma}_{a}^{-1}\right) e-e^\top  \mathbf{M}^\top  \boldsymbol{\Sigma}_{b | a}^{-1} f-f \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M} e+f^\top  \boldsymbol{\Sigma}_{b | a}^{-1} f \nonumber \\
      &=\left(\begin{array}{c}{e} \\ {f}\end{array}\right)^\top \left(\begin{array}{cc}{\mathbf{M}^\top  \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}+\boldsymbol{\Sigma}_{a}^{-1}} & {-\mathbf{M}^\top  \boldsymbol{\Sigma}_{b | a}^{-1}} \\ {-\boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}} & {\boldsymbol{\Sigma}_{b | a}^{-1}}\end{array}\right)\left(\begin{array}{l}{e} \\ {f}\end{array}\right) \nonumber  \\
      &= \left(\begin{array}{c}{\mathbf{x}_{a}-\boldsymbol{\mu}_{a}} \\ {\mathbf{x}_{b}-\mathbf{M} \boldsymbol{\mu}_{a}-\mathbf{d}}\end{array}\right)^\top \mathbf{R}^{-1}\left(\begin{array}{c}{\mathbf{x}_{a}-\boldsymbol{\mu}_{a}} \\ {\mathbf{x}_{b}-\mathbf{M} \boldsymbol{\mu}_{a}-\mathbf{d}}\end{array}\right)
    \end{align}

    By (\ref{eq:app-det-schur-complement}) we have that

    \begin{align}
      \det \mathbf{R}^{-1}&=\det\left(\boldsymbol{\Sigma}_{b | a}^{-1}\right) \det\left(\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}+\boldsymbol{\Sigma}_{a}^{-1}-\mathbf{M}^\top \boldsymbol{\Sigma}_{b | a}^{-1} \boldsymbol{\Sigma}_{b | a} \boldsymbol{\Sigma}_{b | a}^{-1} \mathbf{M}\right) \nonumber  \\
      &=\det\left(\boldsymbol{\Sigma}_{b | a}^{-1}\right) \det\left(\boldsymbol{\Sigma}_{a}^{-1}\right) \nonumber  \\
      &=\frac{1}{\det\left(\boldsymbol{\Sigma}_{b | a}\right) \det\left(\boldsymbol{\Sigma}_{a}\right)}
    \end{align}
    
    Finally by inspection of (\ref{eq:app-affine-transf-cov-matrix}), we take the mean of $\mathbf{x}_b$ as $\mathbf{M} \boldsymbol{\mu}_{a}+\mathbf{d}$ which concludes the proof.
  \end{proof}

  With the results of Theorems 1, 2 and 3 we get the following corollary

  \begin{corollary}

    Being $\mathbf{x}_b$ conditioned on $\mathbf{x}_a$ and Gaussian distributed as

    \begin{equation}
      p\left(x_{a}\right)=\mathcal{N}\left(x_{a} ; \mu_{a}, \Sigma_{a}\right), \quad p\left(x_{b} | x_{a}\right)=\mathcal{N}\left(x_{b} ; M x_{a}+d, \Sigma_{b | a}\right)
    \end{equation}

    with $\mu_{b} =M \mu_{a}+d$, $\Sigma_{b} =\Sigma_{b | a}+M \Sigma_{a} M^{T}$, $\mathbf{M}$ a constant matrix and $\mathbf{d}$ a constant vector, both with the appropriate dimensions. Then conditional distribution $p(\mathbf{x}_a|\mathbf{x}_b)$ is given by

    \begin{subequations}
    
    \begin{align}
      p\left(x_{a} | x_{b}\right)&=\mathcal{N}\left(x_{a} ; \mu_{a | b}, \Sigma_{a | b}\right)
    \end{align}
    with
    \begin{align}
        \mu_{a | b}&=\Sigma_{a | b}\left(M^{T} \Sigma_{b | a}^{-1}\left(x_{b}-b\right)+\Sigma_{a}^{-1} \mu_{a}\right) \\ \Sigma_{a | b}&=\left(\Sigma_{a}^{-1}+M^{T} \Sigma_{b | a}^{-1} M\right)^{-1}.
    \end{align}
  \end{subequations}
  \end{corollary}

\end{appendices}