\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{apalike}
\citation{hennig2013gaussian}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\citation{Rasmussen:2005:GPM:1162254}
\citation{getoor2009}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\newlabel{eq:lin-reg-model-1}{{3.1}{3}{Linear Regression}{equation.3.1}{}}
\newlabel{eq:lin-reg-model-phi}{{3.2}{3}{Linear Regression}{equation.3.2}{}}
\newlabel{eq:lin-reg-model-1-matrix-form}{{3.3}{3}{Linear Regression}{equation.3.3}{}}
\newlabel{eq:lin-reg-error-function-1}{{3.4}{3}{Linear Regression}{equation.3.4}{}}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lin-reg-overfit}{{3}{4}{Plots of polynomials for the model in (\ref {eq:lin-reg-model-1}) having various orders $M$, shown as red curves \cite {Bishop:2006:PRM:1162264}.\relax }{figure.caption.3}{}}
\newlabel{eq:lin-reg-error-function-matrix-form}{{3.6}{4}{Linear Regression}{equation.3.6}{}}
\newlabel{eq:lin-reg-opt-param}{{3.7}{4}{Linear Regression}{equation.3.7}{}}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\newlabel{tab:reg-lin-reg-weights}{{1}{5}{Table of the coefficients $\mathbf {w} ^*$ for polynomials of various order. Observe how the typical magnitude of the coefficients increases dramatically as the order of the polynomial increases \cite {Bishop:2006:PRM:1162264}.\relax }{table.caption.4}{}}
\newlabel{eq:lin-reg-error-function-regularized}{{3.8a}{5}{Regularized Linear Regression}{equation.3.8a}{}}
\newlabel{eq:lin-reg-error-function-regularized-matrix-form}{{3.8b}{5}{Regularized Linear Regression}{equation.3.8b}{}}
\newlabel{fig:reg-lin-reg-overfit-lambda}{{4}{5}{Plots of $M = 9$ polynomials fitted to the data set using the regularized error function (\ref {eq:lin-reg-error-function-regularized}) for two values of the regularization parameter $\lambda $ corresponding to $\ln \lambda = -18$ and $\ln \lambda = 0$. The case of no regularizer, i.e., $\lambda = 0$, corresponding to $\ln \lambda = -\infty $, is shown at the bottom right of \autoref {fig:lin-reg-overfit} \cite {Bishop:2006:PRM:1162264}.\relax }{figure.caption.5}{}}
\newlabel{eq:bay-reg-bayes-rule}{{4.1}{5}{A Bayesian view of Linear Regression}{equation.4.1}{}}
\citation{degroot2012probability}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\newlabel{eq:bay-lin-bayes-theorem}{{4.2}{6}{A Bayesian view of Linear Regression}{equation.4.2}{}}
\newlabel{eq:bay-cur-fit-1}{{4.3}{6}{Bayesian curve fitting}{equation.4.3}{}}
\newlabel{eq:bay-reg-ml-function}{{4.5}{7}{Bayesian curve fitting}{equation.4.5}{}}
\newlabel{eq:bay-lin-reg-prior-dist}{{4.8}{7}{Bayesian curve fitting}{equation.4.8}{}}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\newlabel{eq:bay-inf-posterior}{{4.15}{8}{Bayesian inference}{equation.4.15}{}}
\citation{Rasmussen:2005:GPM:1162254}
\newlabel{eq:intro-poly}{{4.21}{10}{Kernels}{equation.4.21}{}}
\citation{mackay1998introduction}
\citation{Rasmussen:2005:GPM:1162254}
\citation{mackay1998introduction}
\citation{Bishop:2006:PRM:1162264}
\citation{Bishop:2006:PRM:1162264}
\newlabel{eq:gau-pro-mean-cov}{{5.3}{12}{Gaussian processes}{equation.5.3}{}}
\newlabel{subsec:app-matrix-form}{{A.1}{13}{Matrix Form}{subsection.1.A.1}{}}
\citation{graybill1983matrices}
\citation{schon_lindsten}
\newlabel{subsec:app-par-gau}{{B}{14}{Bayes' theorem for Gaussian variables\cite {schon_lindsten}}{Appendix.1.B}{}}
\newlabel{eq:app-par-gau-multivariate-gaussian}{{B.2}{14}{Partitioned Gaussian distributions}{equation.1.B.2}{}}
\newlabel{theorem:app-gau-margin}{{1}{15}{Marginalization}{theorem.1.B.1.1}{}}
\newlabel{eq:app-par-gau-schur}{{B.8}{15}{Partitioned Gaussian distributions}{equation.1.B.8}{}}
\newlabel{eq:app-par-gau-marginalization-1}{{B.9}{16}{Partitioned Gaussian distributions}{equation.1.B.9}{}}
\newlabel{eq:app-par-gau-marginalization-2}{{B.10}{16}{Partitioned Gaussian distributions}{equation.1.B.10}{}}
\newlabel{eq:app-det-schur-complement}{{B.11}{16}{Partitioned Gaussian distributions}{equation.1.B.11}{}}
\newlabel{theorem:app-gau-condit}{{2}{16}{Conditioning}{theorem.1.B.1.2}{}}
\newlabel{subsec:app-affine-transf-gau}{{B.2}{17}{Affine transformation}{subsection.1.B.2}{}}
\newlabel{eq:app-affine-transf-cov-matrix}{{B.29}{18}{Affine transformation}{equation.1.B.29}{}}
\newlabel{cor:aff-marg-cond-gaussian}{{1}{19}{}{corollary.1.B.2.1}{}}
\newlabel{eq:app-cor-affine-b}{{B.33b}{19}{}{equation.1.B.33b}{}}
\bibdata{Intro_to_GP_bibliography}
\bibcite{Bishop:2006:PRM:1162264}{Bishop, 2006}
\bibcite{degroot2012probability}{DeGroot and Schervish, 2012}
\bibcite{getoor2009}{Getoor, 2009}
\bibcite{graybill1983matrices}{Graybill, 1983}
\bibcite{hennig2013gaussian}{Hennig, 2013}
\bibcite{mackay1998introduction}{MacKay, 1998}
\bibcite{Rasmussen:2005:GPM:1162254}{Rasmussen and Williams, 2005}
\bibcite{schon_lindsten}{Sch\IeC {\"o}n and Lindsten, 2011}
