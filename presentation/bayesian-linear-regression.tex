\section{Bayesian Linear Regression}
\framecard{\insertsection}

\begin{frame}{\insertsection}
	Seeking a Bayesian approach, the next steps consists to apply the \textbf{sum} and \textbf{product} rules of probability to evaluate the predictive distribution. By now we assume that the hyperparameters are fixed, but they could assume a distribution too. \\
	\vspace{1em}
	We saw that the posterior distribution for $\mathbf{w}$ could be given by
	\begin{equation*}
		\underbrace{p\left( \mathbf{w} | \mathbf{x}, \mathbf{t} \right)}_{\text{posterior}} \propto \underbrace{p\left(  \mathbf{t} |\mathbf{w} ,\mathbf{x} \right)}_{\text{likelihood}}  \underbrace{p\left( \mathbf{w} \right)}_{\text{prior}}
	\end{equation*}

\end{frame}


\subsection{The D-dimensional Gaussian Distribution}

\begin{frame}{\insertsubsection}

Remember the One-dimensional Gaussian distribution
\begin{block}{One-dimensional Gaussian distribution}
\begin{equation*}
	\mathcal{N}(x | \mu, \sigma^2) = \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left\{ -\frac{1}{2 \sigma^2} (x- \mu)^2 \right\} > 0
\end{equation*}
where $\mu$ is the mean and $\sigma^2$ the variance.
\end{block}
\visible<2->{\vspace{1em}
			First we'll consider a geometrical approach by the quadratic distance $(x -  \mu)^2$ normalized by the variance $\sigma^2$. This comprehension will help us with the D-dimensional case.
   		     	}
\end{frame}

\begin{frame}{\insertsubsection}
	To more than one dimensions, we'll consider the points ($x$) distance for the mean of the distribution, as we done in the one dimensional case, by adding a term to prioritize some dimension distribution in particular. Then
	\begin{equation*}
		\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma} ^{-1} (\mathbf{x} - \boldsymbol{\mu}) 
	\end{equation*}
called \textit{Mahalanobis distance}. And it's becomes the \textit{Euclidean distance}, when $\boldsymbol{\Sigma}$ is the indentity matrix. This means that the all the distances are equally normalized. The matrix $\boldsymbol{\Sigma}$ is the covariance matrix of the distributions, by definition.
\end{frame}

\begin{frame}{\insertsubsection}
And then
\begin{block}{D-dimensional Gaussian distribution}
\begin{equation*}
	\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2 \pi )^{D/2}} \frac{1}{|\boldsymbol{\Sigma}|^{1/2}} \exp \left\{ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma} ^{-1} (\mathbf{x} - \boldsymbol{\mu})  \right\} 
\end{equation*}
where $\boldsymbol{\mu}$ is the D-dimensional mean vector, $\boldsymbol{\Sigma}$ the D$\times$D-dimensional variance matrix and $|\boldsymbol{\Sigma}|$ its determinant.
\end{block}
\end{frame}


\subsection{Bayes' rule for Gaussian variables}

\begin{frame}{\insertsubsection}

To proceed we'd like to prove that the Gaussians are \textbf{closed under linear transformations}. This will allow us to transform the Gaussians under the likelihood distribution given a prior. For example, given a distribution

\begin{equation*}
	p(\mathbf{z}) = p(\mathbf{x},\mathbf{y})
\end{equation*}

\end{frame}

\begin{frame}{\insertsubsection}
\visible<1->{
	In other words, we're trying to find the parginal distribution $p\left( \mathbf{y} \right)$ and the conditional distribution $p\left( \mathbf{x} | \mathbf{y} \right)$, given
	\begin{align*}
	p\left( \mathbf{x} \right) =& \mathcal{N} \left( \mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1} \right) \\
	p\left( \mathbf{y} | \mathbf{x} \right) =& \mathcal{N} \left( \mathbf{y}|\mathbf{Ax}+\mathbf{b}, \mathbf{L}^{-1} \right)
	\end{align*}
}
\visible<2->{
So, applying the joint distribution and the its $\ln$ after
	\begin{align*}
	p\left( \mathbf{z} \right) =& \ p\left(  \mathbf{x}, \mathbf{y} \right) = p\left(  \mathbf{y} | \mathbf{x} \right) p\left( \mathbf{x} \right) \\
	\ln p\left( \mathbf{z} \right) =& \ln p\left(  \mathbf{y} | \mathbf{x} \right) + \ln p\left( \mathbf{x} \right)  \\
					   =&  -\frac{1}{2}\left( \mathbf{x} - \boldsymbol{\mu} \right)^T\boldsymbol{\Lambda}\left( \mathbf{x} - \boldsymbol{\mu} \right) \\ & -\frac{1}{2} \left( \mathbf{y} - \mathbf{Ax} - \mathbf{b} \right)^T \mathbf{L} \left( \mathbf{y} - \mathbf{Ax} - \mathbf{b} \right)+ \text{const}
	\end{align*}
}
\end{frame}

\begin{frame}{\insertsubsection}
\visible<1->{
	The "const" is the term independent of $\mathbf{x}$ and $\mathbf{y}$. Then, expanding the quadratic form
	\begin{align*}
	\ln p\left( \mathbf{z} \right) =&  -\frac{1}{2} \mathbf{x}^T \left( \boldsymbol{\Lambda} + \mathbf{A}^T \mathbf{L} \mathbf{A} \right) \mathbf{x} - \frac{1}{2} \mathbf{y}^T \mathbf{Ly} + \frac{1}{2} \mathbf{y}^T \mathbf{LAx} + \frac{1}{2} \mathbf{x}^T \mathbf{A}^T \mathbf{Ly} \\ 
					   =& - \frac{1}{2} \begin{pmatrix} \mathbf{x} \\ \mathbf{y} \end{pmatrix}^T \begin{pmatrix} \boldsymbol{\Lambda} + \mathbf{A}^T \mathbf{LA} & -\mathbf{A}^T \mathbf{L} \\ -\mathbf{LA} & \mathbf{L} \end{pmatrix}  \begin{pmatrix} \mathbf{x} \\ \mathbf{y} \end{pmatrix} = - \frac{1}{2} \mathbf{z}^T \mathbf{Rz}
	\end{align*}
}
\visible<2->{
	We'll apply the partitioned matrices inversion to obtain $\mathbf{R}^{-1}$
	\begin{align*}
	\mathbf{R}^{-1} =& \begin{pmatrix} \boldsymbol{\Lambda}^{-1} & \boldsymbol{\Lambda}^{-1}\mathbf{A}^T \\ \mathbf{A}\boldsymbol{\Lambda}^{-1} & \mathbf{L}^{-1} + \mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^T \end{pmatrix}
	\end{align*}
}
\end{frame}

\begin{frame}{\insertsubsection}
\visible<1->{
	The expanded form of $\ln p\left( \mathbf{z} \right)$ give us the mean too by the linear terms, then
	\begin{align*}
	\mathbf{x}^T \boldsymbol{\Lambda \mu}  -\mathbf{x}^T \mathbf{A}^T \mathbf{Lb} + \mathbf{y}^T \mathbf{Lb} = \begin{pmatrix} \mathbf{x} \\ \mathbf{y} \end{pmatrix}^T \begin{pmatrix} \boldsymbol{\Lambda \mu} - \mathbf{A}^T \mathbf{Lb} \\ \mathbf{Lb}  \end{pmatrix}
	\end{align*}
}
\visible<2->{
	By inspection of the linear terms
	\begin{align*}
	\mathbb{E}[\mathbf{z}] = \mathbf{R}^{-1} \begin{pmatrix} \boldsymbol{\Lambda \mu} - \mathbf{A}^T \mathbf{Lb} \\ \mathbf{Lb}  \end{pmatrix}
	\end{align*}
}
\end{frame}

\begin{frame}{\insertsubsection}
\visible<1->{
	The expanded form of $\ln p\left( \mathbf{z} \right)$ give us the mean too by the linear terms, then
	\begin{align*}
	\mathbf{x}^T \boldsymbol{\Lambda \mu}  -\mathbf{x}^T \mathbf{A}^T \mathbf{Lb} + \mathbf{y}^T \mathbf{Lb} = \begin{pmatrix} \mathbf{x} \\ \mathbf{y} \end{pmatrix}^T \begin{pmatrix} \boldsymbol{\Lambda \mu} - \mathbf{A}^T \mathbf{Lb} \\ \mathbf{Lb}  \end{pmatrix}
	\end{align*}
}
\visible<2->{
	By inspection of the linear terms
	\begin{align*}
	\mathbb{E}[\mathbf{z}] = \mathbf{R}^{-1} \begin{pmatrix} \boldsymbol{\Lambda \mu} - \mathbf{A}^T \mathbf{Lb} \\ \mathbf{Lb}  \end{pmatrix} =  \begin{pmatrix} \boldsymbol{\mu}  \\  \mathbf{A}\boldsymbol{\mu} + \mathbf{b}  \end{pmatrix}
	\end{align*}
}
\end{frame}

\begin{frame}{\insertsubsection}
And then we we'll have that
\visible<1->{
	\begin{align*}
	\mathbb{E}[\mathbf{y}] =& \ \mathbf{A}\boldsymbol{\mu} + \mathbf{b} \\
	\text{cov}[\mathbf{y}] =& \ \mathbf{L}^{-1} + \mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^T
	\end{align*}
}
\visible<2->{
	\begin{align*}
	\mathbb{E}[\mathbf{x}|\mathbf{y}] =& \ \left(  \boldsymbol{\Lambda} + \mathbf{A}^T \mathbf{LA} \right)^{-1} \left\{ \mathbf{A}^T \mathbf{L}(\mathbf{y}- \mathbf{b}) + \boldsymbol{\Lambda \mu} \right\} \\
	\text{cov}[\mathbf{x}|\mathbf{y}] =& \ \left(  \boldsymbol{\Lambda} + \mathbf{A}^T \mathbf{LA} \right)^{-1}
	\end{align*}
}
\end{frame}

\begin{frame}{\insertsection}
In the next step, we'll assume a \textbf{prior distribution over parameters}, $p(\mathbf{w})$, and define it as a Gaussian distribution, then
\begin{equation*}
	p(\mathbf{w}) = \mathcal{N} \left( \mathbf{w} | \mathbf{m}_0, \mathbf{S}_0 \right)
\end{equation*}	
with mean $\mathbf{m}_0$ and variance $\mathbf{S}_0$.
\end{frame}

% \begin{frame}{\insertsubsection}
% 
% \begin{block}{Partitioned Gaussians}
% Given a joint Gaussian distribution $\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma})$ with $\boldsymbol{\Lambda} \equiv \boldsymbol{\Sigma}^{-1}$ and
% \begin{equation*}
% 	\mathbf{x} = 
% 	 \begin{pmatrix}
% 		\mathbf{x} _{a} \\
% 		\mathbf{x} _{b}
% 	\end{pmatrix} , 
% 	\boldsymbol{\mu} = 
% 	 \begin{pmatrix}
% 		\boldsymbol{\mu}_{a} \\
% 		\boldsymbol{\mu}_{b}
% 	\end{pmatrix} , 
% 	 \boldsymbol{\Sigma} = 
% 	 \begin{pmatrix}
% 		\boldsymbol{\Sigma}_{aa} & \boldsymbol{\Sigma}_{ab} \\
% 		\boldsymbol{\Sigma}_{ba} & \boldsymbol{\Sigma}_{bb} 
% 	\end{pmatrix} ,  \boldsymbol{\Lambda} = 
% 	 \begin{pmatrix}
% 		\boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab} \\
% 		\boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb} 
% 	\end{pmatrix}.
% \end{equation*}
% Will give us \\
% \begin{itemize}
% \item \textcolor{UniGold}{Conditional distribution:}
% \begin{equation*}
% p\left( \mathbf{x}_a | \mathbf{x}_b \right) = \mathcal{N}\left( \mathbf{x}_a| \boldsymbol{\mu}_{a|b}, \boldsymbol{\Lambda}^{-1}_{aa} \right), \  \boldsymbol{\mu}_{a|b} =  \boldsymbol{\mu}_a - \boldsymbol{\Lambda}^{-1}_{aa}\boldsymbol{\Lambda}_{ab} \left( \mathbf{x}_b - \boldsymbol{\mu}_b \right)
% \end{equation*}

% \item \textcolor{UniGold}{Marginal distribution:}
% \begin{equation*}
% p\left( \mathbf{x}_a \right) = \mathcal{N} \left( \mathbf{x}_a | \boldsymbol{\mu}_a,\boldsymbol{\Sigma}_{aa} \right)
% \end{equation*}
% \end{itemize}

% \end{block}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{\insertsubsection}

\begin{block}{Marginal and Conditioned Gaussians}

\begin{itemize}
\item \textcolor{UniGold}{For $\mathbf{y}$ given $\mathbf{x}$:}
\begin{align*}
	p\left( \mathbf{x} \right) =& \mathcal{N} \left( \mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1} \right) \\
	p\left( \mathbf{y} | \mathbf{x} \right) =& \mathcal{N} \left( \mathbf{y}|\mathbf{Ax}+\mathbf{b}, \mathbf{L}^{-1} \right)
\end{align*}

\item \textcolor{UniGold}{For $\mathbf{x}$ given $\mathbf{y}$:}
\begin{align*}
p\left( \mathbf{x} | \mathbf{y} \right) =& \ \mathcal{N}\left( \mathbf{y}| ,  \boldsymbol{\Sigma} \left\{ \mathbf{A}^T \mathbf{L}(\mathbf{y}-\mathbf{b} + \boldsymbol{\Sigma \mu}) \right\}, \boldsymbol{\Sigma} \right) \\
p\left( \mathbf{y} \right) =& \ \mathcal{N}\left( \mathbf{y}|\mathbf{A} \boldsymbol{\mu} + \mathbf{b}, \mathbf{L}^{-1} + \mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^T \right) \text{, where } \boldsymbol{\Sigma} = \left(  \boldsymbol{\Lambda} + \mathbf{A}^T \mathbf{LA} \right)^{-1}
\end{align*}
\end{itemize}

\end{block}
\end{frame}

\begin{frame}{\insertsubsection}
	By the derivations, we make the assumptions of given $p(\mathbf{w})$ and  for $p(\mathbf{t}|\mathbf{w})$ such that 
	\begin{align*}
		p(\mathbf{t}|\mathbf{w}) &= \mathcal{N}\left( \mathbf{t}|y(\mathbf{x},\mathbf{w}), \beta^{-1} \right)	\\
		&= \mathcal{N}\left( \mathbf{t}|\boldsymbol{\Phi}^T\mathbf{w}, \beta^{-1} \right)
	\end{align*}
	And then $p(\mathbf{w}|\mathbf{t}) = \mathcal{N}\left( \mathbf{w}|\mathbf{m}_N, \mathbf{S}_N \right)$ where
	\begin{align*}
		\mathbf{m}_N &= \mathbf{S}_N \left( \mathbf{S}_0^{-1}\mathbf{m}_0 + \beta \boldsymbol{\Phi}^T\mathbf{t}  \right) \\
		\mathbf{S}_N^{-1} &= \mathbf{S}_0^{-1} + \beta\boldsymbol{\Phi}^T\boldsymbol{\Phi}
	\end{align*}
\end{frame}

% \begin{frame}{\insertsubsection}
% 	We'll make the same optimization we made for the mono-variate case. So the logarithmic likelihood results 
% 	\begin{block}{Maximum likelihood for the Gaussian}
% 	\begin{equation*}
% 	\ln p \left( \mathbf{X} | \boldsymbol{\mu}, \boldsymbol{\Sigma} \right) =
% 	- \frac{N D}{2} \ln (2 \pi) - \frac{N}{2} \ln |\boldsymbol{\Sigma}| - 
% 	\frac{1}{2} \sum^N_{n=1} \left( \mathbf{x}_n - \boldsymbol{\mu} \right)^T 
% 	\Sigma^{-1} \left( \mathbf{x}_n - \boldsymbol{\mu} \right)
% 	\end{equation*}	
% 	\end{block}

% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


































