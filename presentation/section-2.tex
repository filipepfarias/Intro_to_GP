\section{Linear Regression}\label{sec:linear-regression}
\framecard{\insertsection}
\subsection{Curve Fitting}

\begin{frame}{\insertsubsection}

\visible<1->{If we have a data set of points in the space that comes from observations of an experiment, may we want to predict other points in unknown parameters. This could be done with \textbf{\textcolor{UniGold}{ curve fitting }}. So the given points.}

\vspace{0.5em}


\visible<2->{We're supposing that our experiment could be modeled somehow, i.e it don't be totally random. So we could define some strategy to find our model.}

\visible<3->{
	\begin{block}{Strategy}
	\begin{itemize}
		\item[1] Purpose a model, e.g. functions like exponential, polynomial and others.
		\item[2] Train our model with the training data set, finding the unknown parameters.
	\end{itemize}
	\end{block}
}

\end{frame}


\begin{frame}{\insertsubsection}

Let's fit the example by polynomial curve fitting

	\begin{exampleblock}{Example}
	\begin{figure}
	\label{fig:plot-fitting-example}
		\includegraphics[totalheight=0.5\textheight]{Figure1c2.pdf}
		\end{figure}
	\end{exampleblock}

\end{frame}

\begin{frame}{\insertsubsection}

Be the model chosen 
\visible<2->{
	\begin{align}
		y(x,\mathbf{w}) &= w_0 + w_1x + w_2x^2  + ... + w_Mx^M \nonumber \\
					& = \sum^M_{j=0} w_j x^j
	\end{align}
}
\visible<3->{

So we'll try to minimize the mean squared error between the model and the training data set. So, if the MSE is defined as
}
\visible<4->{
\begin{align}
	E(\mathbf{w}) \triangleq \frac{1}{2} \sum_{n=1}^N \left\{ \hat{y}_n -  y_n \right\}^2
\end{align}

where $\hat{y}$ is our predict.
}
\end{frame}

\begin{frame}{\insertsubsection}
	\begin{figure}
		\includegraphics[totalheight=0.7\textheight]{Figure1c3.pdf}
	\end{figure}
\end{frame}

\begin{frame}{\insertsubsection}
So, we evaluate the error between our model and the target $\mathbf{t}$, that is our training data.
\begin{align}
	E(\mathbf{w}) =& \frac{1}{2} \sum_{n=1}^N \left\{ y_n(x,\mathbf{w}) -  t_n \right\}^2
\end{align}

And try to minimize it by

\begin{align}
	\frac{\partial}{\partial \mathbf{w}} E(\mathbf{w}) &= 0 \nonumber \\
	\sum_{n=1}^N \frac{\partial}{\partial \mathbf{w}} y(x,\mathbf{w}) \left\{  y_n(x,\mathbf{w}) -  t_n \right\} &= 0
\end{align}

\end{frame}

\begin{frame}{\insertsubsection}

\begin{equation}
\begin{bmatrix}

\frac{\partial}{\partial w_1} y(x,\mathbf{w}) \left\{  y(x,\mathbf{w}) -  t_n \right\} \\
\frac{\partial}{\partial w_2} y(x,\mathbf{w}) \left\{  y(x,\mathbf{w}) -  t_n \right\} \\
... \\
\frac{\partial}{\partial w_M} y(x,\mathbf{w}) \left\{  y(x,\mathbf{w}) -  t_n \right\} \\

\end{bmatrix} = 
\end{equation}

\end{frame}


%
%

\subsection{Bayesian Curve Fitting}\label{bayesian-curve-fitting}
\begin{frame}{\insertsubsection}
	\visible<1->{So, we'll start to look the regression with a statistical approach. To encourage you, let's take the sentence.}
	\visible<2>{\vspace{1.5em}
		\begin{block}{Sentence}
		\textit{If we could update the \textbf{\textcolor{red}{regression weights}} as we acquire some new values of the experiment?}
		\end{block}
   		     }
\end{frame}

\begin{frame}{\insertsubsection}
	\visible<1->{Let's take a look again at the Bayes Theorem}
	
	\visible<2->{
	\begin{block}{Bayes Theorem}{
			\begin{equation}\label{bayes_theorem}
				\visible<2->{ p(\mathbf{w}|\mathcal{D}) = \frac{p(\mathcal{D}| \mathbf{w}) \overbrace{ p(\mathbf{w})}^{\mathclap{\text{the weights probability}}}} { \underbrace{ p(\mathcal{D})}_{\mathclap{\text{the data probability}}}} } 
			\end{equation}
			}
	\end{block}
	\visible<3->{So, if\textbf{ we have the probability} of the data, we'll could estimate the\textbf{ future weights}.}
	\visible<4>{\centering\textbf{\textcolor{red}{But, how?}}}
	}
\end{frame}

\begin{frame}{\insertsubsection}
	\visible<1->{Taking some steps back, let's re-visit the \textbf{Curve Fitting}. There, the strategy was minimize the error function.\vspace{1.5em} \\}
	\visible<2->{Now we'll try to view the same problem with a \textit{probabilistic perspective}. We're trying to make predictions for the target value $\mathbf{t}$ given some new values of $x$.\vspace{1.5em} \\}
	\visible<3->{A good ideia is to express our target values $\mathbf{t}$ in terms of \textbf{gaussians distributions} with the mean equals to $y(x,\mathbf{w})$.}
\end{frame}

\begin{frame}{\insertsubsection}
	\begin{figure}
		\label{fig:schematic-gaussian-distribution}
		\includegraphics[totalheight=0.6\textheight]{Figure1c16.pdf}
		\caption{Schematic of the polynomial function $y(x,\mathbf{w})$ and the gaussian distribution $p$.}

	\end{figure}
\end{frame}

\begin{frame}{\insertsubsection}
	By the \ref{fig:schematic-gaussian-distribution}, we assume the relation
	\visible<2->{
	\begin{equation}
		p(t|x,\mathbf{w},\beta) = \mathcal{N}(t|y(x,\mathbf{w}),\beta^{-1})
	\end{equation}}
	\visible<3->{and then, assume that the training data $\{ \mathbf{x}, \mathbf{t} \}$ is indenpendent and identically distributed (i.i.d.) and put on \textbf{product form}, i.e. the joint probability is}
	\begin{align} \visible<3->{
		p( \mathbf{t}| \mathbf{x}, \mathbf{w}, \beta) &= \mathcal{N}(t_0|y(x_1, \mathbf{w}), \beta^{-1}) \cap \mathcal{N}(t_n|y(x_0, \mathbf{w}), \beta^{-1}) ... \cap \mathcal{N}(t_n|y(x_0, \mathbf{w}), \beta^{-1}) }\\ 
		\visible<4->{
		&=\prod^N_{n=1} \mathcal{N}(t_n|y(x_n, \mathbf{w}), \beta^{-1})
		}
	\end{align}
	\visible<4->{
	regarding that $\beta^{-1} = \sigma^2$.}
\end{frame}

\begin{frame}{\insertsubsection}
	And we'll have a function to maximize if we apply the logarithm function to $p$, so 
	\visible<2->{
	\begin{equation}
		\ln \left( p( \mathbf{t}| \mathbf{x}, \mathbf{w}, \beta) \right) = \sum^N_{n=1} \ln \left( \mathcal{N}(t_n|y(x_n, \mathbf{w}), \beta^{-1}) \right)
	\end{equation}}
	\visible<3->{
	Applying the \textbf{Gaussian distribution} (see \ref{eq:gaussian-distribution}) will result
	\begin{equation}
		\ln \left( p( \mathbf{t}| \mathbf{x}, \mathbf{w}, \beta) \right) = -\frac{\beta}{2} \sum^N_{n=1} \left\{ y(x_n,\mathbf{w} -t_n ) \right\}^2 + \frac{N}{2}\ln (\beta) - \frac{N}{2} \ln (2 \pi)
	\end{equation}
	}
\end{frame}

\begin{frame}{\insertsubsection}
	And taking the derivatives with respect to $\beta$ to minimize the error
	
		\begin{align}
		\visible<2->{
			\frac{\partial}{\partial \beta}\ln \left( p( \mathbf{t}| \mathbf{x}, \mathbf{w}, \beta) \right) &=0 } \\
		\visible<3->{
			 -\frac{1}{2} \sum^N_{n=1} \left\{ y(x_n,\mathbf{w} -t_n ) \right\}^2 + \frac{N}{2}\frac{1}{\beta} &= 0 \\  }
		\visible<4->{
			 \frac{1}{N} \sum^N_{n=1} \left\{ y(x_n,\mathbf{w} -t_n ) \right\}^2  &= \frac{1}{\beta_{ML}}  }
		\end{align}
	\visible<4->{
	Where $\beta_{ML}$ is the maximum likelihood.}

\end{frame}

%\begin{frame}{Title}
%  $
%    blah =
%    \only<1>{blah}
%    \only<2>{result}
%    \visible<1>{%
%      =
%      \begin{cases}
%          blah \\ 
%          blah
%        \end{cases}
%      }
%    $
%\end{frame}